{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2017d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from train_utils import *\n",
    "from eval_utils import *\n",
    "from train import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae15ee60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']=\"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES']= \"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825232b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "available_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29685eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"use_cuda: {use_cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c24cb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global\n",
    "overwrite_files = False\n",
    "datasets = [\"cifar\", \"purchase\", \"texas\"]\n",
    "random_seeds = list(np.arange(5, 15))\n",
    "best_valid_acc = False\n",
    "# denotes best batch size per config\n",
    "selected_epoch = {\n",
    "    \"cifar\": {\n",
    "        \"weighted_erm\": 24, # 128\n",
    "        \"mmd\": {\"warmup\": 7, \"no-warmup\": 14}, # 512\n",
    "        \"adv_reg\": {\"no-ref_term\": 24, \"ref_term\": 24} # 128\n",
    "    },\n",
    "    \"purchase\": {\n",
    "        \"weighted_erm\": 19, # 512\n",
    "        \"mmd\": {\"warmup\": 19, \"no-warmup\": 24}, # 512\n",
    "        \"adv_reg\": {\"no-ref_term\": 9, \"ref_term\": 34} # 128\n",
    "    },\n",
    "    \"texas\": {\n",
    "        \"weighted_erm\": 3, # 128\n",
    "        \"mmd\": {\"warmup\": 7, \"no-warmup\": 7}, # 512\n",
    "        \"adv_reg\": {\"no-ref_term\": 9, \"ref_term\": 19} # 128 , 10 / 18\n",
    "    },\n",
    "}\n",
    "\n",
    "# weighted erm\n",
    "weight_props = {\n",
    "    \"cifar\": [0.5, 0.7, 0.9, 0.97, 0.995, 1.],\n",
    "    \"purchase\": [0.5, 0.7, 0.9, 0.97, 0.98, 1.],\n",
    "    \"texas\": [0.5, 0.7, 0.9, 0.97, 0.999, 1.],\n",
    "}\n",
    "batch_sizes = [128, 512]\n",
    "\n",
    "# mmd regularization\n",
    "mmd_weights = [0.1, 0.2, 0.35, 0.7, 1.5]\n",
    "mmd_versions = [\"no-warmup\", \"warmup\"]\n",
    "\n",
    "# AdvReg\n",
    "alphas = {\n",
    "    \"cifar\": [1e-6, 1e-3, 1e-1, 1.],\n",
    "    \"purchase\": [1., 2., 3., 6., 10., 20.],\n",
    "    \"texas\": [1., 2., 3., 6., 10., 20.]\n",
    "}\n",
    "adv_reg_versions = [\"no-ref_term\", \"ref_term\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17e25c5",
   "metadata": {},
   "source": [
    "### Weighted ERM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dddf5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_erm_metrics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550e077f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    for weight_prop in weight_props[dataset]:\n",
    "        for batch_size in batch_sizes:\n",
    "            for rttr in [1.0]:\n",
    "                for validation_metric in [\"valid_acc\"]:\n",
    "                    for random_seed in random_seeds:\n",
    "                        run_name = f\"weight{weight_prop}-rttr{rttr}\"\n",
    "                        if batch_size == 512:\n",
    "                            run_name += \"-bs512\"\n",
    "                        folder = f\"{dataset}_checkpoints/seed{random_seed}/weighted-erm/train-{run_name}\"\n",
    "                        if os.path.isdir(folder) is False:\n",
    "                            print(f\"not found: {folder}\")\n",
    "                            continue\n",
    "                        else:\n",
    "                            print(f\"found: {folder}\")                        \n",
    "                        \n",
    "                        if dataset == \"texas\":\n",
    "                            epochs = 8\n",
    "                        elif dataset == \"purchase\":\n",
    "                            epochs = 20\n",
    "                        elif dataset == \"cifar\":\n",
    "                            epochs = 25\n",
    "                        else:\n",
    "                            raise ValueError(\"unhandled dataset\")\n",
    "                        run_name_to_save = \"weighted_erm-bs512\" if batch_size == 512 else \"weighted_erm\"\n",
    "                        for epoch in range(epochs):\n",
    "                            filename = f\"{folder}/Depoch{epoch}\"\n",
    "                            if os.path.isfile(filename) is False:\n",
    "                                continue \n",
    "                            run_metrics = torch.load(open(filename, \"rb\"))\n",
    "                            run_metrics = {i:(v.item() if isinstance(run_metrics[i], torch.Tensor) else v) for i, v in run_metrics.items()}\n",
    "                            run_metrics[\"run_name\"] = run_name_to_save\n",
    "                            run_metrics[\"seed\"] = random_seed\n",
    "                            run_metrics[\"dataset\"] = dataset\n",
    "                            run_metrics[\"alpha\"] = weight_prop\n",
    "                            run_metrics[\"validation_metric\"] = validation_metric\n",
    "                            run_metrics[\"selected_epoch\"] = selected_epoch[dataset][\"weighted_erm\"]\n",
    "                            weighted_erm_metrics.append(run_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759c9c19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w_erm_df = pd.DataFrame(weighted_erm_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f157cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_erm_by_epoch_df = w_erm_df.groupby([\"dataset\", \"run_name\", \"epoch\", \"alpha\", \"selected_epoch\"])[\n",
    "    \"test_acc\", \"train_acc\", \"valid_acc\", \"conf_acc_train\", \"conf_acc_ref\"].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0abd7a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if best_valid_acc:\n",
    "    w_erm_by_run_df = w_erm_by_epoch_df.groupby([\"dataset\", \"run_name\", \"alpha\"]).apply(\n",
    "        lambda x: x.loc[x[\"valid_acc\"].idxmax()]).reset_index(drop=True)\n",
    "else:\n",
    "    w_erm_by_run_df = w_erm_by_epoch_df.groupby([\"dataset\", \"run_name\", \"alpha\"]).apply(\n",
    "        lambda x: x[x[\"epoch\"] == x[\"selected_epoch\"]]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f20a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get NN attack results\n",
    "if best_valid_acc:\n",
    "    nn_attack_results = []\n",
    "    for dataset in datasets:\n",
    "        for weight_prop in weight_props[dataset]:\n",
    "            for batch_size in batch_sizes:\n",
    "                for rttr in [1.0]:\n",
    "                    for random_seed in random_seeds:\n",
    "                        run_name = f\"weight{weight_prop}-rttr{rttr}\"\n",
    "                        if batch_size == 512:\n",
    "                            run_name += \"-bs512\"\n",
    "                        folder = f\"{dataset}_checkpoints/seed{random_seed}/weighted-erm/train-{run_name}\"\n",
    "                        filename = f\"{folder}/best_valid_acc_nn_attack\"\n",
    "                        if os.path.isfile(filename) is False:\n",
    "                            continue   \n",
    "                        nn_attack_metrics = torch.load(open(filename, \"rb\"))\n",
    "                        \n",
    "                        # ensure old runs aren't collected\n",
    "                        if \"epoch_train\" in nn_attack_metrics:\n",
    "                            continue\n",
    "                        \n",
    "                        run_name_to_save = \"weighted_erm-bs512\" if batch_size == 512 else \"weighted_erm\"\n",
    "                        nn_attack_metrics[\"dataset\"] = dataset\n",
    "                        nn_attack_metrics[\"seed\"] = random_seed\n",
    "                        nn_attack_metrics[\"run_name\"] = run_name_to_save\n",
    "                        nn_attack_metrics[\"alpha\"] = weight_prop\n",
    "                        nn_attack_metrics\n",
    "                        nn_attack_results.append(nn_attack_metrics)\n",
    "\n",
    "    nn_attack_df = pd.DataFrame(nn_attack_results).rename(\n",
    "        columns={\n",
    "            \"best_acc_train\": \"nn_acc_train\", \n",
    "            \"best_acc_ref\": \"nn_acc_ref\"}\n",
    "    )\n",
    "    nn_attack_df = nn_attack_df.groupby(\n",
    "        [\"dataset\", \"run_name\", \"alpha\"])[[\"nn_acc_train\", \"nn_acc_ref\"]].mean().reset_index()    \n",
    "    w_erm_by_run_df = w_erm_by_run_df.merge(nn_attack_df, how=\"left\", on=[\"dataset\", \"run_name\", \"alpha\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83ccbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get WERM-ES df\n",
    "\n",
    "w_erm_es_df = pd.concat([\n",
    "    w_erm_by_epoch_df[\n",
    "        (w_erm_by_epoch_df[\"dataset\"] == \"purchase\") &\n",
    "        (w_erm_by_epoch_df[\"run_name\"] == \"weighted_erm-bs512\") & \n",
    "        (w_erm_by_epoch_df[\"epoch\"] == 6)\n",
    "    ],\n",
    "    w_erm_by_epoch_df[\n",
    "        (w_erm_by_epoch_df[\"dataset\"] == \"texas\") &\n",
    "        (w_erm_by_epoch_df[\"run_name\"] == \"weighted_erm\") & \n",
    "        (w_erm_by_epoch_df[\"epoch\"] == 0)\n",
    "    ],\n",
    "    w_erm_by_epoch_df[\n",
    "        (w_erm_by_epoch_df[\"dataset\"] == \"cifar\") &\n",
    "        (w_erm_by_epoch_df[\"run_name\"] == \"weighted_erm\") & \n",
    "        (w_erm_by_epoch_df[\"epoch\"] == 5)\n",
    "    ]\n",
    "])\n",
    "w_erm_es_df[\"run_name\"] = \"weighted_erm-es\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd541fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_erm_by_run_df = pd.concat([w_erm_by_run_df, w_erm_es_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83456e75",
   "metadata": {},
   "source": [
    "### MMD Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea16923",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmd_metrics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e7fc76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    for mmd_weight in mmd_weights:\n",
    "        for mmd_version in mmd_versions:\n",
    "            for rttr in [1.0]:\n",
    "                for validation_metric in [\"valid_acc\"]:\n",
    "                    for random_seed in random_seeds:\n",
    "                        if dataset != \"cifar\" and mmd_weight == 1e-4:\n",
    "                            continue\n",
    "                        \n",
    "                        run_name = f\"weight{mmd_weight}-rttr{rttr}\"\n",
    "                        if mmd_version == \"no-warmup\":\n",
    "                            run_name += \"-no-warmup\"\n",
    "                        folder = f\"{dataset}_checkpoints/seed{random_seed}/mmd-regularization/train-{run_name}\"\n",
    "                        if os.path.isdir(folder) is False:\n",
    "                            print(f\"not found: {folder}\")\n",
    "                            continue\n",
    "                        else:\n",
    "                            print(f\"found: {folder}\") \n",
    "                    \n",
    "                        if dataset == \"texas\":\n",
    "                            if mmd_version == \"no-warmup\":\n",
    "                                epochs = 16\n",
    "                            else:\n",
    "                                epochs = 8\n",
    "                        elif dataset == \"purchase\":\n",
    "                            if mmd_version == \"no-warmup\":\n",
    "                                epochs = 40\n",
    "                            else:\n",
    "                                epochs = 20\n",
    "                        elif dataset == \"cifar\":\n",
    "                            if mmd_version == \"no-warmup\":\n",
    "                                epochs = 16\n",
    "                            else:\n",
    "                                epochs = 8\n",
    "                        else:\n",
    "                            raise ValueError(\"unhandled dataset\")\n",
    "                        \n",
    "                        run_name_to_save = \"mmd-no-warmup\" if mmd_version == \"no-warmup\" else \"mmd\"\n",
    "                        for epoch in range(epochs):\n",
    "                            run_metrics = torch.load(open(f\"{folder}/Depoch{epoch}\", \"rb\"))\n",
    "                            run_metrics = {i:(v.item() if isinstance(run_metrics[i], torch.Tensor) else v) for i, v in run_metrics.items()}\n",
    "                            run_metrics[\"run_name\"] = run_name_to_save\n",
    "                            run_metrics[\"seed\"] = random_seed\n",
    "                            run_metrics[\"dataset\"] = dataset\n",
    "                            run_metrics[\"alpha\"] = mmd_weight\n",
    "                            run_metrics[\"validation_metric\"] = validation_metric\n",
    "                            run_metrics[\"selected_epoch\"] = selected_epoch[dataset][\"mmd\"][mmd_version]\n",
    "                            mmd_metrics.append(run_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ebc4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmd_df = pd.DataFrame(mmd_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bada1235",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmd_by_epoch_df = mmd_df.groupby([\"dataset\", \"run_name\", \"epoch\", \"alpha\", \"selected_epoch\"])[\n",
    "    \"test_acc\", \"valid_acc\", \"conf_acc_train\", \"conf_acc_ref\"].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a096747",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if best_valid_acc:\n",
    "    mmd_by_run_df = mmd_by_epoch_df.groupby([\"dataset\", \"run_name\", \"alpha\"]).apply(\n",
    "        lambda x: x.loc[x[\"valid_acc\"].idxmax()]).reset_index(drop=True)\n",
    "else:\n",
    "    mmd_by_run_df = mmd_by_epoch_df.groupby([\"dataset\", \"run_name\", \"alpha\"]).apply(\n",
    "        lambda x: x[x[\"epoch\"] == x[\"selected_epoch\"]]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef283fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get NN attack results\n",
    "if best_valid_acc:\n",
    "    nn_attack_results = []\n",
    "\n",
    "    for dataset in datasets:\n",
    "        for mmd_weight in mmd_weights:\n",
    "            for mmd_version in mmd_versions:\n",
    "                for rttr in [1.0]:\n",
    "                    for random_seed in random_seeds:\n",
    "                        run_name = f\"weight{mmd_weight}-rttr{rttr}\"\n",
    "                        if mmd_version == \"no-warmup\":\n",
    "                            run_name += \"-no-warmup\"\n",
    "                        folder = f\"{dataset}_checkpoints/seed{random_seed}/mmd-regularization/train-{run_name}\"\n",
    "                        filename = f\"{folder}/best_valid_acc_nn_attack\"\n",
    "                        if os.path.isfile(filename) is False:\n",
    "                            continue \n",
    "                        nn_attack_metrics = torch.load(open(filename, \"rb\"))\n",
    "                        \n",
    "                        # ensure old runs aren't collected\n",
    "                        if \"epoch_train\" in nn_attack_metrics:\n",
    "                            continue\n",
    "                        \n",
    "                        run_name_to_save = \"mmd-no-warmup\" if mmd_version == \"no-warmup\" else \"mmd\"\n",
    "                        nn_attack_metrics[\"dataset\"] = dataset\n",
    "                        nn_attack_metrics[\"seed\"] = random_seed\n",
    "                        nn_attack_metrics[\"run_name\"] = run_name_to_save\n",
    "                        nn_attack_metrics[\"alpha\"] = mmd_weight\n",
    "                        nn_attack_metrics\n",
    "                        nn_attack_results.append(nn_attack_metrics)\n",
    "\n",
    "    nn_attack_df = pd.DataFrame(nn_attack_results).rename(\n",
    "        columns={\n",
    "            \"best_acc_train\": \"nn_acc_train\", \n",
    "            \"best_acc_ref\": \"nn_acc_ref\"}\n",
    "    )\n",
    "    nn_attack_df = nn_attack_df.groupby(\n",
    "        [\"dataset\", \"run_name\", \"alpha\"])[[\"nn_acc_train\", \"nn_acc_ref\"]].mean().reset_index()        \n",
    "    mmd_by_run_df = mmd_by_run_df.merge(nn_attack_df, how=\"left\", on=[\"dataset\", \"run_name\", \"alpha\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1930b9",
   "metadata": {},
   "source": [
    "### AdvReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1408582f",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_reg_metrics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449cbaa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    for alpha in alphas[dataset]:\n",
    "        for adv_reg_version in adv_reg_versions:\n",
    "            for rttr in [1.0]:\n",
    "                for validation_metric in [\"valid_acc\"]:\n",
    "                    for random_seed in random_seeds:\n",
    "                        if adv_reg_version == \"ref_term\":\n",
    "                            version_tag = \"nf\"\n",
    "                        elif adv_reg_version == \"no-ref_term\":\n",
    "                            version_tag = \"of\"\n",
    "                        else:\n",
    "                            raise ValueError(\"unhandled adv_reg_version\")\n",
    "                            \n",
    "                        if dataset == \"texas\":\n",
    "                            epochs = 20\n",
    "                        elif dataset == \"purchase\":\n",
    "                            epochs = 40\n",
    "                        elif dataset == \"cifar\":\n",
    "                            epochs = 30\n",
    "                        else:\n",
    "                            raise ValueError(\"unhandled dataset\")\n",
    "                            \n",
    "                        run_name = f\"coin_flip-mse-{version_tag}-{rttr}-{epochs}-20-1\"\n",
    "                        \n",
    "                        folder = f\"{dataset}_checkpoints/seed{random_seed}/alpha{alpha}/train-{run_name}\"\n",
    "                        if os.path.isdir(folder) is False:\n",
    "                            print(f\"not found: {folder}\")\n",
    "                            continue\n",
    "                        else:\n",
    "                            print(f\"found: {folder}\") \n",
    "                    \n",
    "                        run_name_to_save = \"adv_reg-ref_term\" if adv_reg_version == \"ref_term\" else \"adv_reg\"\n",
    "                        for epoch in range(epochs):\n",
    "                            filename = f\"{folder}/Depoch{epoch}\"\n",
    "                            if os.path.isfile(filename) is False:\n",
    "                                continue \n",
    "                            run_metrics = torch.load(open(filename, \"rb\"))\n",
    "                            run_metrics = {i:(v.item() if isinstance(run_metrics[i], torch.Tensor) else v) for i, v in run_metrics.items()}\n",
    "                            run_metrics[\"run_name\"] = run_name_to_save\n",
    "                            run_metrics[\"seed\"] = random_seed\n",
    "                            run_metrics[\"dataset\"] = dataset\n",
    "                            run_metrics[\"alpha\"] = alpha\n",
    "                            run_metrics[\"validation_metric\"] = validation_metric\n",
    "                            run_metrics[\"selected_epoch\"] = selected_epoch[dataset][\"adv_reg\"][adv_reg_version]\n",
    "                            adv_reg_metrics.append(run_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888bb9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_reg_df = pd.DataFrame(adv_reg_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ba029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_reg_by_epoch_df = adv_reg_df.groupby([\"dataset\", \"run_name\", \"epoch\", \"alpha\", \"selected_epoch\"])[\n",
    "    \"test_acc\", \"valid_acc\", \"conf_acc_train\", \"conf_acc_ref\"].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f280d638",
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_valid_acc:\n",
    "    adv_reg_by_run_df = adv_reg_by_epoch_df.groupby([\"dataset\", \"run_name\", \"alpha\"]).apply(\n",
    "        lambda x: x.loc[x[\"valid_acc\"].idxmax()]).reset_index(drop=True)\n",
    "else:\n",
    "    adv_reg_by_run_df = adv_reg_by_epoch_df.groupby([\"dataset\", \"run_name\", \"alpha\"]).apply(\n",
    "        lambda x: x[x[\"epoch\"] == x[\"selected_epoch\"]]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e221d923",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get NN attack results\n",
    "if best_valid_acc:\n",
    "    nn_attack_results = []\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        for alpha in alphas[dataset]:\n",
    "            for adv_reg_version in adv_reg_versions:\n",
    "                for rttr in [1.0]:\n",
    "                    for random_seed in random_seeds:\n",
    "                        if adv_reg_version == \"ref_term\":\n",
    "                            version_tag = \"nf\"\n",
    "                        elif adv_reg_version == \"no-ref_term\":\n",
    "                            version_tag = \"of\"\n",
    "                        else:\n",
    "                            raise ValueError(\"unhandled adv_reg_version\")\n",
    "                            \n",
    "                        if dataset == \"texas\":\n",
    "                            epochs = 20\n",
    "                        elif dataset == \"purchase\":\n",
    "                            epochs = 40\n",
    "                        elif dataset == \"cifar\":\n",
    "                            epochs = 30\n",
    "                        else:\n",
    "                            raise ValueError(\"unhandled dataset\")\n",
    "\n",
    "                        run_name = f\"coin_flip-mse-{version_tag}-{rttr}-{epochs}-20-1\"\n",
    "                        folder = f\"{dataset}_checkpoints/seed{random_seed}/alpha{alpha}/train-{run_name}\"\n",
    "                        filename = f\"{folder}/best_valid_acc_nn_attack\"\n",
    "                        if os.path.isfile(filename) is False:\n",
    "                            continue \n",
    "                        nn_attack_metrics = torch.load(open(filename, \"rb\"))\n",
    "                        \n",
    "                        # ensure old runs aren't collected\n",
    "                        if \"epoch_train\" in nn_attack_metrics:\n",
    "                            continue\n",
    "                        \n",
    "                        run_name_to_save = \"adv_reg-ref_term\" if adv_reg_version == \"ref_term\" else \"adv_reg\"\n",
    "                        nn_attack_metrics[\"dataset\"] = dataset\n",
    "                        nn_attack_metrics[\"seed\"] = random_seed\n",
    "                        nn_attack_metrics[\"run_name\"] = run_name_to_save\n",
    "                        nn_attack_metrics[\"alpha\"] = alpha\n",
    "                        nn_attack_metrics\n",
    "                        nn_attack_results.append(nn_attack_metrics)\n",
    "\n",
    "    nn_attack_df = pd.DataFrame(nn_attack_results).rename(\n",
    "        columns={\n",
    "            \"best_acc_train\": \"nn_acc_train\", \n",
    "            \"best_acc_ref\": \"nn_acc_ref\"}\n",
    "    )\n",
    "    nn_attack_df = nn_attack_df.groupby(\n",
    "        [\"dataset\", \"run_name\", \"alpha\"])[[\"nn_acc_train\", \"nn_acc_ref\"]].mean().reset_index() \n",
    "    adv_reg_by_run_df = adv_reg_by_run_df.merge(nn_attack_df, how=\"left\", on=[\"dataset\", \"run_name\", \"alpha\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e86ac8",
   "metadata": {},
   "source": [
    "### preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6f791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_run_df = pd.concat([w_erm_by_run_df, mmd_by_run_df, adv_reg_by_run_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61efbd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_acc_train_df = by_run_df[[\"dataset\", \"run_name\", \"alpha\", \"test_acc\", \"conf_acc_train\"]].rename(columns={\"conf_acc_train\": \"conf_acc\"})\n",
    "conf_acc_train_df[\"target_dataset\"] = [\"training\"] * len(conf_acc_train_df)\n",
    "conf_acc_ref_df = by_run_df[[\"dataset\", \"run_name\", \"alpha\", \"test_acc\", \"conf_acc_ref\"]].rename(columns={\"conf_acc_ref\": \"conf_acc\"})\n",
    "conf_acc_ref_df[\"target_dataset\"] = [\"reference\"] * len(conf_acc_ref_df)\n",
    "twin_plot_df = pd.concat([conf_acc_train_df, conf_acc_ref_df]).reset_index(drop=True)\n",
    "twin_plot_df[\"test_acc\"] = twin_plot_df[\"test_acc\"] / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df300a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_valid_acc:\n",
    "    nn_acc_train_df = by_run_df[[\"dataset\", \"run_name\", \"alpha\", \"test_acc\", \"nn_acc_train\"]].rename(columns={\"nn_acc_train\": \"nn_acc\"})\n",
    "    nn_acc_train_df[\"target_dataset\"] = [\"training\"] * len(nn_acc_train_df)\n",
    "    nn_acc_ref_df = by_run_df[[\"dataset\", \"run_name\", \"alpha\", \"test_acc\", \"nn_acc_ref\"]].rename(columns={\"nn_acc_ref\": \"nn_acc\"})\n",
    "    nn_acc_ref_df[\"target_dataset\"] = [\"reference\"] * len(nn_acc_ref_df)\n",
    "    twin_plot_nn_df = pd.concat([nn_acc_train_df, nn_acc_ref_df]).reset_index(drop=True)\n",
    "    twin_plot_nn_df[\"test_acc\"] = twin_plot_nn_df[\"test_acc\"] / 100\n",
    "    twin_plot_nn_df = twin_plot_nn_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40cb1cc",
   "metadata": {},
   "source": [
    "### Figure 3 Plot (including WERM-ES) / Table 2 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b34f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1, figsize=(16, 18))\n",
    "markers = {\"training\": \"s\", \"reference\": \"X\"}\n",
    "color_palette = sns.color_palette(\"colorblind\", 5)\n",
    "color_map = {\n",
    "    'WERM': color_palette[0],\n",
    "    'MMD': color_palette[1],\n",
    "    'AdvReg': color_palette[2],\n",
    "    'AdvReg-RT': color_palette[3],\n",
    "    'WERM-ES': color_palette[4],\n",
    "}\n",
    "\n",
    "# purchase\n",
    "purchase_run_names = {\n",
    "    \"weighted_erm\": \"weighted_erm-bs128\",\n",
    "    \"weighted_erm-es\": \"WERM-ES\",\n",
    "    \"weighted_erm-bs512\": \"WERM\",\n",
    "    \"mmd\": \"MMD\",\n",
    "    \"mmd-no-warmup\": \"mmd-no-warmup\",\n",
    "    \"adv_reg\": \"AdvReg\", \n",
    "    \"adv_reg-ref_term\": \"AdvReg-RT\", \n",
    "}\n",
    "\n",
    "twin_plot_purchase_df = twin_plot_df[twin_plot_df[\"dataset\"] == \"purchase\"]\n",
    "twin_plot_purchase_df[\"run_name\"] = twin_plot_purchase_df[\"run_name\"].apply(\n",
    "    lambda x: purchase_run_names[x] if x in purchase_run_names else x)\n",
    "twin_plot_purchase_df[\"Defense Method\"] = twin_plot_purchase_df[\"run_name\"]\n",
    "twin_plot_purchase_df[\"Target Dataset\"] = twin_plot_purchase_df[\"target_dataset\"]\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=twin_plot_purchase_df[\n",
    "        (twin_plot_purchase_df[\"test_acc\"] > 0.6) & \n",
    "        ~(twin_plot_purchase_df[\"run_name\"].isin(\n",
    "            [\"mmd-no-warmup\", \"weighted_erm-bs128\"]))#, \"WERM-ES\"])\n",
    "\n",
    "    ], \n",
    "    x=\"test_acc\", y=\"conf_acc\", hue=\"Defense Method\", style=\"Target Dataset\", markers=markers,\n",
    "    palette=color_map, marker=10, s=100, ax=ax[0])\n",
    "ax[0].set_ylabel(\"MIA Accuracy\", fontsize=12)\n",
    "ax[0].set_xlabel(\"Test Accuracy\", fontsize=12)\n",
    "ax[0].set_title(\"Purchase100 - Comprehensive Tradeoff Analysis\", fontsize=14)\n",
    "\n",
    "# texas\n",
    "texas_run_names = {\n",
    "    \"weighted_erm\": \"WERM\",\n",
    "    \"weighted_erm-es\": \"WERM-ES\",\n",
    "    \"weighted_erm-bs512\": \"weighted_erm-bs512\",\n",
    "    \"mmd\": \"mmd-warmup\",\n",
    "    \"mmd-no-warmup\": \"MMD\",\n",
    "    \"adv_reg\": \"AdvReg\", \n",
    "    \"adv_reg-ref_term\": \"AdvReg-RT\",\n",
    "}\n",
    "\n",
    "twin_plot_texas_df = twin_plot_df[twin_plot_df[\"dataset\"] == \"texas\"]\n",
    "twin_plot_texas_df[\"run_name\"] = twin_plot_texas_df[\"run_name\"].apply(\n",
    "    lambda x: texas_run_names[x] if x in texas_run_names else x)\n",
    "twin_plot_texas_df[\"Defense Method\"] = twin_plot_texas_df[\"run_name\"]\n",
    "twin_plot_texas_df[\"Target Dataset\"] = twin_plot_texas_df[\"target_dataset\"]\n",
    "\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=twin_plot_texas_df[\n",
    "        (twin_plot_texas_df[\"test_acc\"] > 0.35) & \n",
    "        ~(twin_plot_texas_df[\"run_name\"].isin([\"mmd-warmup\", \"weighted_erm-bs512\"]))#, \"WERM-ES\"]))\n",
    "    ], \n",
    "    x=\"test_acc\", y=\"conf_acc\", hue=\"Defense Method\", style=\"Target Dataset\", markers=markers,\n",
    "    palette=color_map, marker=10, s=100, ax=ax[1])\n",
    "ax[1].set_ylabel(\"MIA Accuracy\", fontsize=12)\n",
    "ax[1].set_xlabel(\"Test Accuracy\", fontsize=12)\n",
    "ax[1].set_title(\"Texas100 - Comprehensive Tradeoff Analysis\", fontsize=14)\n",
    "\n",
    "# cifar\n",
    "cifar_run_names = {\n",
    "    \"weighted_erm\": \"WERM\",\n",
    "    \"weighted_erm-es\": \"WERM-ES\",\n",
    "    \"weighted_erm-bs512\": \"weighted_erm-bs512\",\n",
    "    \"mmd\": \"mmd-warmup\",\n",
    "    \"mmd-no-warmup\": \"MMD\",\n",
    "    \"adv_reg\": \"AdvReg\", \n",
    "    \"adv_reg-ref_term\": \"AdvReg-RT\",\n",
    "}\n",
    "twin_plot_cifar_df = twin_plot_df[twin_plot_df[\"dataset\"] == \"cifar\"]\n",
    "twin_plot_cifar_df[\"run_name\"] = twin_plot_cifar_df[\"run_name\"].apply(\n",
    "    lambda x: cifar_run_names[x] if x in cifar_run_names else x)\n",
    "twin_plot_cifar_df[\"Defense Method\"] = twin_plot_cifar_df[\"run_name\"]\n",
    "twin_plot_cifar_df[\"Target Dataset\"] = twin_plot_cifar_df[\"target_dataset\"]\n",
    "\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=twin_plot_cifar_df[\n",
    "        (twin_plot_cifar_df[\"dataset\"] == \"cifar\") & \n",
    "        ~(twin_plot_cifar_df[\"run_name\"].isin([\"mmd-warmup\", \"weighted_erm-bs512\"]))#, \"WERM-ES\"]))\n",
    "    ], \n",
    "    x=\"test_acc\", y=\"conf_acc\", hue=\"Defense Method\", style=\"Target Dataset\", markers=markers,\n",
    "    palette=color_map, marker=10, s=100, ax=ax[2])\n",
    "ax[2].set_ylabel(\"MIA Accuracy\", fontsize=12)\n",
    "ax[2].set_xlabel(\"Test Accuracy\", fontsize=12)\n",
    "ax[2].set_title(\"CIFAR100 - Comprehensive Tradeoff Analysis\", fontsize=14)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5df2fd",
   "metadata": {},
   "source": [
    "### Pearson Correlation Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577dd243",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_run_df2 = deepcopy(by_run_df)\n",
    "by_run_df2[\"emp_priv_ratio\"] = (by_run_df2[\"conf_acc_train\"] - 0.5) / (by_run_df2[\"conf_acc_ref\"] - 0.5)\n",
    "by_run_df2[\"the_priv_ratio\"] = by_run_df2[\"alpha\"] / (1 - by_run_df2[\"alpha\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a681cd18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_names = {\n",
    "    \"purchase\": {\n",
    "        \"weighted_erm\": \"weighted_erm-bs128\",\n",
    "        \"weighted_erm-es\": \"WERM-ES\",\n",
    "        \"weighted_erm-bs512\": \"WERM\",\n",
    "        \"mmd\": \"MMD\",\n",
    "        \"mmd-no-warmup\": \"mmd-no-warmup\",\n",
    "        \"adv_reg\": \"AdvReg\", \n",
    "        \"adv_reg-ref_term\": \"AdvReg-RT\", \n",
    "    },\n",
    "    \"texas\": {\n",
    "        \"weighted_erm\": \"WERM\",\n",
    "        \"weighted_erm-es\": \"WERM-ES\",\n",
    "        \"weighted_erm-bs512\": \"weighted_erm-bs512\",\n",
    "        \"mmd\": \"mmd-warmup\",\n",
    "        \"mmd-no-warmup\": \"MMD\",\n",
    "        \"adv_reg\": \"AdvReg\", \n",
    "        \"adv_reg-ref_term\": \"AdvReg-RT\",\n",
    "    },\n",
    "    \"cifar\": {\n",
    "        \"weighted_erm\": \"WERM\",\n",
    "        \"weighted_erm-es\": \"WERM-ES\",\n",
    "        \"weighted_erm-bs512\": \"weighted_erm-bs512\",\n",
    "        \"mmd\": \"mmd-warmup\",\n",
    "        \"mmd-no-warmup\": \"MMD\",\n",
    "        \"adv_reg\": \"AdvReg\", \n",
    "        \"adv_reg-ref_term\": \"AdvReg-RT\",\n",
    "    }\n",
    "}\n",
    "\n",
    "def change_run_name(row):\n",
    "    row_dataset = row[\"dataset\"]\n",
    "    return run_names[row_dataset][row[\"run_name\"]]\n",
    "\n",
    "by_run_df2[\"run_name\"] = by_run_df2.apply(change_run_name, axis=1)\n",
    "by_run_df2 = by_run_df2[~(by_run_df2[\"run_name\"].isin(\n",
    "    [\"weighted_erm-bs512\", \"weighted_erm-bs128\", \"mmd-warmup\", \"mmd-no-warmup\"]\n",
    "))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fb8b60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "by_run_df2 = by_run_df2[~((by_run_df2[\"alpha\"] == 1.) & (by_run_df2[\"run_name\"].isin([\"WERM\", \"WERM-ES\"])))]\n",
    "by_run_df2[\"conf_acc_ref\"] = by_run_df2[\"conf_acc_ref\"].apply(lambda x: 0.501 if x < 0.5 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc9a1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_run_df2[\"emp_priv_ratio\"] = (by_run_df2[\"conf_acc_train\"] - 0.5) / (by_run_df2[\"conf_acc_ref\"] - 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d05269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_run_df2[\"the_priv_ratio\"] = by_run_df2.apply(\n",
    "    lambda row: row[\"alpha\"] / (1 - row[\"alpha\"]) if row[\"run_name\"] in [\"WERM\", \"WERM-ES\"] else 1 / row[\"alpha\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce91a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only include alpha values that are common among all datasets for overall analysis\n",
    "werm_corrcoef_relative_privacy = np.corrcoef(\n",
    "    by_run_df2_ds[(by_run_df2_ds[\"run_name\"] == \"WERM\") & ~(by_run_df2_ds[\"alpha\"].isin([0.98, 0.999, 0.995]))][\"emp_priv_ratio\"], \n",
    "    by_run_df2_ds[(by_run_df2_ds[\"run_name\"] == \"WERM\") & ~(by_run_df2_ds[\"alpha\"].isin([0.98, 0.999, 0.995]))][\"the_priv_ratio\"]\n",
    ")\n",
    "\n",
    "for dataset in datasets:\n",
    "    pcc_ds = np.corrcoef(\n",
    "        by_run_df2_ds[(by_run_df2_ds[\"dataset\"] == dataset) & (by_run_df2_ds[\"run_name\"] == \"WERM\") & \n",
    "                      ~(by_run_df2_ds[\"alpha\"].isin([0.98, 0.999, 0.995]))][\"emp_priv_ratio\"], \n",
    "        by_run_df2_ds[(by_run_df2_ds[\"dataset\"] == dataset) & (by_run_df2_ds[\"run_name\"] == \"WERM\") & \n",
    "                      ~(by_run_df2_ds[\"alpha\"].isin([0.98, 0.999, 0.995]))][\"the_priv_ratio\"]\n",
    "    )[0][1]\n",
    "    print(f\"WERM PCC {dataset}: {pcc_ds}\")\n",
    "print(f\"WERM PCC Overall: {werm_corrcoef_relative_privacy[0][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25236d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmd_corrcoef_relative_privacy = np.corrcoef(\n",
    "    by_run_df2_ds[(by_run_df2_ds[\"run_name\"] == \"MMD\") & (by_run_df2_ds[\"emp_priv_ratio\"] != np.inf)][\"emp_priv_ratio\"], \n",
    "    by_run_df2_ds[(by_run_df2_ds[\"run_name\"] == \"MMD\") & (by_run_df2_ds[\"emp_priv_ratio\"] != np.inf)][\"the_priv_ratio\"]\n",
    ")\n",
    "for dataset in datasets:\n",
    "    pcc_ds = np.corrcoef(\n",
    "        by_run_df2_ds[(by_run_df2_ds[\"dataset\"] == dataset) & (by_run_df2_ds[\"run_name\"] == \"MMD\") & (by_run_df2_ds[\"emp_priv_ratio\"] != np.inf)\n",
    "                     ][\"emp_priv_ratio\"], \n",
    "        by_run_df2_ds[(by_run_df2_ds[\"dataset\"] == dataset) & (by_run_df2_ds[\"run_name\"] == \"MMD\") & (by_run_df2_ds[\"emp_priv_ratio\"] != np.inf)\n",
    "                     ][\"the_priv_ratio\"]\n",
    "    )[0][1]\n",
    "    print(f\"MMD PCC {dataset}: {pcc_ds}\")\n",
    "print(f\"MMD PCC Overall: {mmd_corrcoef_relative_privacy[0][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7fe05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "advreg_corrcoef_relative_privacy = np.corrcoef(\n",
    "    by_run_df2_ds[(by_run_df2_ds[\"run_name\"] == \"AdvReg\") & (by_run_df2_ds[\"emp_priv_ratio\"] != np.inf)][\"emp_priv_ratio\"], \n",
    "    by_run_df2_ds[(by_run_df2_ds[\"run_name\"] == \"AdvReg\") & (by_run_df2_ds[\"emp_priv_ratio\"] != np.inf)][\"the_priv_ratio\"]\n",
    ")\n",
    "for dataset in datasets:\n",
    "    pcc_ds = np.corrcoef(\n",
    "        by_run_df2_ds[(by_run_df2_ds[\"dataset\"] == dataset) & (by_run_df2_ds[\"run_name\"] == \"AdvReg\") & \n",
    "                      (by_run_df2_ds[\"emp_priv_ratio\"] != np.inf)][\"emp_priv_ratio\"], \n",
    "        by_run_df2_ds[(by_run_df2_ds[\"dataset\"] == dataset) & (by_run_df2_ds[\"run_name\"] == \"AdvReg\") & \n",
    "                      (by_run_df2_ds[\"emp_priv_ratio\"] != np.inf)][\"the_priv_ratio\"]\n",
    "    )[0][1]\n",
    "    print(f\"AdvReg PCC {dataset}: {pcc_ds}\")\n",
    "print(f\"AdvReg PCC Overall: {advreg_corrcoef_relative_privacy[0][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27108b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134c6b50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
