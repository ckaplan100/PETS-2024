{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4cc60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from train_utils import *\n",
    "from eval_utils import *\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912fd967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']=\"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES']= \"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37b58b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "available_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e364fdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"use_cuda: {use_cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3992ff",
   "metadata": {},
   "source": [
    "### run and save experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2a9c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_datasets = [\"purchase\", \"texas\", \"cifar\"]\n",
    "random_seeds = list(np.arange(5, 15))\n",
    "use_validation = True\n",
    "weight_props = {\n",
    "    \"purchase\": [0.5, 0.7, 0.9, 0.97, 0.98, 1.],\n",
    "    \"texas\": [0.5, 0.7, 0.9, 0.97, 0.999, 1.],\n",
    "    \"cifar\": [0.5, 0.7, 0.9, 0.97, 0.9975, 1.],\n",
    "}\n",
    "ref_to_train_ratio = 1.0\n",
    "batch_sizes = [512, 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e56011c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in run_datasets:\n",
    "    for random_seed in random_seeds:\n",
    "        for weight_prop in weight_props[dataset]:\n",
    "            for batch_size in batch_sizes:                \n",
    "                set_seed(random_seed)\n",
    "                \n",
    "                if dataset == \"texas\":\n",
    "                    epochs = 4\n",
    "                    num_features = 6169\n",
    "                    train_classifier_ratio = 0.15\n",
    "                    train_test_ratio = 0.4\n",
    "                    train_attack_ratio = train_classifier_ratio * ref_to_train_ratio\n",
    "                    train_valid_ratio = 1 - train_classifier_ratio - train_attack_ratio - train_test_ratio\n",
    "                    data_is_numpy = True\n",
    "                elif dataset == \"purchase\":\n",
    "                    epochs = 20\n",
    "                    num_features = 600\n",
    "                    train_classifier_ratio = 0.1\n",
    "                    train_test_ratio = 0.4\n",
    "                    train_attack_ratio = train_classifier_ratio * ref_to_train_ratio\n",
    "                    train_valid_ratio = 1 - train_classifier_ratio - train_attack_ratio - train_test_ratio\n",
    "                    data_is_numpy = True\n",
    "                elif dataset == \"cifar\":\n",
    "                    epochs = 25\n",
    "                    train_classifier_ratio, train_attack_ratio, train_valid_ratio = None, None, None\n",
    "                    data_is_numpy = False\n",
    "                else:\n",
    "                    raise ValueError(\"not handled dataset\")\n",
    "                   \n",
    "                run_name = f\"weight{weight_prop}-rttr{ref_to_train_ratio}\"\n",
    "                if batch_size != 128:\n",
    "                    run_name += f\"-bs{batch_size}\"\n",
    "                print(dataset, random_seed, run_name)\n",
    "\n",
    "                best_valid_acc_state_dict = None\n",
    "                best_total_valid_loss_state_dict = None\n",
    "                best_valid_acc = 0.\n",
    "                best_valid_acc_epoch = -1\n",
    "                best_valid_loss = 1e5\n",
    "                best_valid_loss_epoch = -1\n",
    "\n",
    "                if random_seed in [5, 10]:\n",
    "                    load_randomization = True\n",
    "                else:\n",
    "                    load_randomization = False\n",
    "\n",
    "                train_classifier_data, train_classifier_label, train_attack_data, train_attack_label, valid_data, valid_label, test_data, test_label = load_data(\n",
    "                    dataset=dataset, load_randomization=load_randomization, use_validation=use_validation,\n",
    "                    train_classifier_ratio=train_classifier_ratio, \n",
    "                    train_attack_ratio=train_attack_ratio, \n",
    "                    train_valid_ratio=train_valid_ratio\n",
    "                )\n",
    "                if dataset == \"cifar\":\n",
    "                    model = resnet18(pretrained=False)\n",
    "                    model.fc = nn.Linear(512, 100)\n",
    "                else:\n",
    "                    model = TabularClassifier(num_features=num_features)\n",
    "    #             print(model.features[0].weight)\n",
    "                model = torch.nn.DataParallel(model).cuda()\n",
    "                criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "                optimizer = optim.Adam(model.parameters(), lr=0.001)                \n",
    "                \n",
    "                for epoch in range(epochs):\n",
    "                    if dataset in [\"purchase\", \"texas\"]:\n",
    "                        train_classifier_data_tensor = torch.from_numpy(train_classifier_data).type(torch.FloatTensor)\n",
    "                        train_classifier_label_tensor = torch.from_numpy(train_classifier_label).type(torch.LongTensor)\n",
    "\n",
    "                        train_attack_data_tensor = torch.from_numpy(train_attack_data).type(torch.FloatTensor)\n",
    "                        train_attack_label_tensor = torch.from_numpy(train_attack_label).type(torch.LongTensor)\n",
    "\n",
    "                        valid_data_tensor = torch.from_numpy(valid_data).type(torch.FloatTensor)\n",
    "                        valid_label_tensor = torch.from_numpy(valid_label).type(torch.LongTensor)\n",
    "\n",
    "                        test_data_tensor = torch.from_numpy(test_data).type(torch.FloatTensor)\n",
    "                        test_label_tensor = torch.from_numpy(test_label).type(torch.LongTensor)\n",
    "                    elif dataset == \"cifar\":\n",
    "                        train_classifier_data_tensor = train_classifier_data.type(torch.FloatTensor)\n",
    "                        train_classifier_label_tensor = train_classifier_label.type(torch.LongTensor)\n",
    "\n",
    "                        train_attack_data_tensor = train_attack_data.type(torch.FloatTensor)\n",
    "                        train_attack_label_tensor = train_attack_label.type(torch.LongTensor)\n",
    "\n",
    "                        valid_data_tensor = valid_data.type(torch.FloatTensor)\n",
    "                        valid_label_tensor = valid_label.type(torch.LongTensor)\n",
    "\n",
    "                        test_data_tensor = test_data.type(torch.FloatTensor)\n",
    "                        test_label_tensor = test_label.type(torch.LongTensor)\n",
    "                    else:\n",
    "                        raise ValueError(\"unhandled dataset\")\n",
    "\n",
    "                    train_data_comb_tensor = torch.cat([train_classifier_data_tensor, train_attack_data_tensor])\n",
    "                    train_label_comb_tensor = torch.cat([train_classifier_label_tensor, train_attack_label_tensor])\n",
    "\n",
    "                    weight_prop_train_tensor = torch.ones(len(train_classifier_data_tensor)).cuda() * weight_prop\n",
    "                    weight_prop_attack_tensor = torch.ones(len(train_attack_data_tensor)).cuda() * (1 - weight_prop)\n",
    "\n",
    "                    weight_prop_comb_tensor = torch.cat([weight_prop_train_tensor, weight_prop_attack_tensor])\n",
    "\n",
    "                    r = np.arange(len(train_data_comb_tensor))\n",
    "                    np.random.shuffle(r)\n",
    "\n",
    "                    train_data_comb_tensor = train_data_comb_tensor[r]\n",
    "                    train_label_comb_tensor = train_label_comb_tensor[r]\n",
    "                    weight_prop_comb_tensor = weight_prop_comb_tensor[r]\n",
    "\n",
    "#                     print('\\nEpoch: [%d | %d]' % (epoch, epochs))\n",
    "\n",
    "                    # train with weights\n",
    "                    train_loss, train_acc = train(train_data_comb_tensor, train_label_comb_tensor, model, criterion, optimizer,\n",
    "                                                  batch_size, epoch, use_cuda, loss_weights=weight_prop_comb_tensor)\n",
    "\n",
    "                    # get loss with data splits\n",
    "                    train_loss, train_acc = test(train_classifier_data_tensor, train_classifier_label_tensor, model, criterion, 128, epoch, use_cuda)\n",
    "\n",
    "                    ref_loss, ref_acc = test(train_attack_data_tensor, train_attack_label_tensor, model, criterion, 128, epoch, use_cuda)\n",
    "\n",
    "                    valid_loss, valid_acc = test(valid_data_tensor, valid_label_tensor, model, criterion, 128, epoch, use_cuda)\n",
    "\n",
    "                    test_loss, test_acc = test(test_data_tensor, test_label_tensor, model, criterion, 128, epoch, use_cuda)\n",
    "\n",
    "                    # get privacy attack metrics per epoch\n",
    "                    # test attack eval - training data\n",
    "                    corr_acc_train, conf_acc_train, entr_acc_train, mod_entr_acc_train = evaluation_metrics(\n",
    "                        model, train_classifier_data, train_classifier_label, test_data, test_label, data_is_numpy)\n",
    "\n",
    "                    # test attack eval - ref data\n",
    "                    corr_acc_ref, conf_acc_ref, entr_acc_ref, mod_entr_acc_ref = evaluation_metrics(\n",
    "                        model, train_attack_data, train_attack_label, test_data, test_label, data_is_numpy)\n",
    "\n",
    "#                     print (f'Train Acc: {train_acc}, Ref Acc: {ref_acc}, Valid Acc: {valid_acc}, Test Acc: {test_acc}')\n",
    "#                     print (f'Train Loss: {train_loss}, Ref Loss: {ref_loss}, Valid Loss: {valid_loss}, Test Loss: {test_loss}')\n",
    "#                     print(f\"Conf Attack Train: {conf_acc_train}, Conf Attack Ref: {conf_acc_ref}\")              \n",
    "    #                 print(f'Gap Attack: {1/2 + (train_acc / 100 - test_acc / 100) / 2}')\n",
    "\n",
    "                    filename = f'seed{random_seed}/weighted-erm/train-{run_name}'\n",
    "\n",
    "                    if valid_acc.item() > best_valid_acc:\n",
    "                        best_valid_acc = valid_acc.item()\n",
    "                        best_valid_acc_epoch = epoch\n",
    "                        best_valid_acc_state_dict = deepcopy(model.state_dict())\n",
    "\n",
    "                    if valid_loss.item() < best_valid_loss:\n",
    "                        best_valid_loss = valid_loss.item()\n",
    "                        best_valid_loss_epoch = epoch\n",
    "                        best_total_valid_loss_state_dict = deepcopy(model.state_dict())\n",
    "\n",
    "                    save_checkpoint({        \n",
    "                            'epoch': epoch,\n",
    "                            'test_acc': test_acc,\n",
    "                            'test_loss': test_loss,\n",
    "                            'train_acc': train_acc,\n",
    "                            'train_loss': train_loss,\n",
    "                            'valid_acc': valid_acc,\n",
    "                            'valid_loss': valid_loss,\n",
    "                            'ref_acc': ref_acc,\n",
    "                            'ref_loss': ref_loss,\n",
    "                            'conf_acc_train': conf_acc_train,\n",
    "                            'conf_acc_ref': conf_acc_ref\n",
    "                        }, filename=filename, filename_end='Depoch%d'%epoch, checkpoint=f'./{dataset}_checkpoints')\n",
    "\n",
    "                # save best models\n",
    "                save_checkpoint(\n",
    "                    {\"state_dict\": best_valid_acc_state_dict}, \n",
    "                    checkpoint=f'./{dataset}_checkpoints',\n",
    "                    filename=filename,\n",
    "                    filename_end='best_valid_acc_model'\n",
    "                )\n",
    "                save_checkpoint(\n",
    "                    {\"state_dict\": best_total_valid_loss_state_dict}, \n",
    "                    checkpoint=f'./{dataset}_checkpoints',\n",
    "                    filename=filename,\n",
    "                    filename_end='best_valid_total_loss_model'\n",
    "                )\n",
    "\n",
    "#                 print(f\"Best Valid Acc: {best_valid_acc}, Epoch: {best_valid_acc_epoch}\")\n",
    "#                 print(f\"Best Valid Loss: {best_valid_loss}, Epoch: {best_valid_loss_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d782dab",
   "metadata": {},
   "source": [
    "### evaluate per epoch training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d93a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_datasets = [\"purchase\", \"texas\", \"cifar\"]\n",
    "random_seeds = list(np.arange(5, 15))\n",
    "use_validation = True\n",
    "weight_props = {\n",
    "    \"purchase\": [0.7],\n",
    "    \"texas\": [0.7],\n",
    "    \"cifar\": [0.7],\n",
    "}\n",
    "ref_to_train_ratio = 1.0\n",
    "batch_sizes = [512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fb5d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_per_epoch_werm = {\"cifar\": [], \"purchase\": [], \"texas\": []}\n",
    "for dataset in run_datasets:\n",
    "    for random_seed in random_seeds:\n",
    "        for weight_prop in weight_props[dataset]:\n",
    "            for batch_size in batch_sizes:                \n",
    "                set_seed(random_seed)\n",
    "                \n",
    "                if dataset == \"texas\":\n",
    "                    epochs = 1\n",
    "                    num_features = 6169\n",
    "                    train_classifier_ratio = 0.15\n",
    "                    train_test_ratio = 0.4\n",
    "                    train_attack_ratio = train_classifier_ratio * ref_to_train_ratio\n",
    "                    train_valid_ratio = 1 - train_classifier_ratio - train_attack_ratio - train_test_ratio\n",
    "                    data_is_numpy = True\n",
    "                elif dataset == \"purchase\":\n",
    "                    epochs = 1\n",
    "                    num_features = 600\n",
    "                    train_classifier_ratio = 0.1\n",
    "                    train_test_ratio = 0.4\n",
    "                    train_attack_ratio = train_classifier_ratio * ref_to_train_ratio\n",
    "                    train_valid_ratio = 1 - train_classifier_ratio - train_attack_ratio - train_test_ratio\n",
    "                    data_is_numpy = True\n",
    "                elif dataset == \"cifar\":\n",
    "                    epochs = 1\n",
    "                    train_classifier_ratio, train_attack_ratio, train_valid_ratio = None, None, None\n",
    "                    data_is_numpy = False\n",
    "                else:\n",
    "                    raise ValueError(\"not handled dataset\")\n",
    "                   \n",
    "                run_name = f\"weight{weight_prop}-rttr{ref_to_train_ratio}\"\n",
    "                if batch_size != 128:\n",
    "                    run_name += f\"-bs{batch_size}\"\n",
    "                print(dataset, random_seed, run_name)\n",
    "\n",
    "                best_valid_acc_state_dict = None\n",
    "                best_total_valid_loss_state_dict = None\n",
    "                best_valid_acc = 0.\n",
    "                best_valid_acc_epoch = -1\n",
    "                best_valid_loss = 1e5\n",
    "                best_valid_loss_epoch = -1\n",
    "\n",
    "                if random_seed in [5, 10]:\n",
    "                    load_randomization = True\n",
    "                else:\n",
    "                    load_randomization = False\n",
    "\n",
    "                train_classifier_data, train_classifier_label, train_attack_data, train_attack_label, valid_data, valid_label, test_data, test_label = load_data(\n",
    "                    dataset=dataset, load_randomization=load_randomization, use_validation=use_validation,\n",
    "                    train_classifier_ratio=train_classifier_ratio, \n",
    "                    train_attack_ratio=train_attack_ratio, \n",
    "                    train_valid_ratio=train_valid_ratio\n",
    "                )\n",
    "                if dataset == \"cifar\":\n",
    "                    model = resnet18(pretrained=False)\n",
    "                    model.fc = nn.Linear(512, 100)\n",
    "                else:\n",
    "                    model = TabularClassifier(num_features=num_features)\n",
    "    #             print(model.features[0].weight)\n",
    "                model = torch.nn.DataParallel(model).cuda()\n",
    "                criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "                optimizer = optim.Adam(model.parameters(), lr=0.001)                \n",
    "                \n",
    "                start_time_werm = time.perf_counter()\n",
    "                for epoch in range(epochs):\n",
    "\n",
    "                    if dataset in [\"purchase\", \"texas\"]:\n",
    "                        train_classifier_data_tensor = torch.from_numpy(train_classifier_data).type(torch.FloatTensor)\n",
    "                        train_classifier_label_tensor = torch.from_numpy(train_classifier_label).type(torch.LongTensor)\n",
    "\n",
    "                        train_attack_data_tensor = torch.from_numpy(train_attack_data).type(torch.FloatTensor)\n",
    "                        train_attack_label_tensor = torch.from_numpy(train_attack_label).type(torch.LongTensor)\n",
    "\n",
    "                        valid_data_tensor = torch.from_numpy(valid_data).type(torch.FloatTensor)\n",
    "                        valid_label_tensor = torch.from_numpy(valid_label).type(torch.LongTensor)\n",
    "\n",
    "                        test_data_tensor = torch.from_numpy(test_data).type(torch.FloatTensor)\n",
    "                        test_label_tensor = torch.from_numpy(test_label).type(torch.LongTensor)\n",
    "                    elif dataset == \"cifar\":\n",
    "                        train_classifier_data_tensor = train_classifier_data.type(torch.FloatTensor)\n",
    "                        train_classifier_label_tensor = train_classifier_label.type(torch.LongTensor)\n",
    "\n",
    "                        train_attack_data_tensor = train_attack_data.type(torch.FloatTensor)\n",
    "                        train_attack_label_tensor = train_attack_label.type(torch.LongTensor)\n",
    "\n",
    "                        valid_data_tensor = valid_data.type(torch.FloatTensor)\n",
    "                        valid_label_tensor = valid_label.type(torch.LongTensor)\n",
    "\n",
    "                        test_data_tensor = test_data.type(torch.FloatTensor)\n",
    "                        test_label_tensor = test_label.type(torch.LongTensor)\n",
    "                    else:\n",
    "                        raise ValueError(\"unhandled dataset\")\n",
    "\n",
    "                    train_data_comb_tensor = torch.cat([train_classifier_data_tensor, train_attack_data_tensor])\n",
    "                    train_label_comb_tensor = torch.cat([train_classifier_label_tensor, train_attack_label_tensor])\n",
    "\n",
    "                    weight_prop_train_tensor = torch.ones(len(train_classifier_data_tensor)).cuda() * weight_prop\n",
    "                    weight_prop_attack_tensor = torch.ones(len(train_attack_data_tensor)).cuda() * (1 - weight_prop)\n",
    "\n",
    "                    weight_prop_comb_tensor = torch.cat([weight_prop_train_tensor, weight_prop_attack_tensor])\n",
    "\n",
    "                    r = np.arange(len(train_data_comb_tensor))\n",
    "                    np.random.shuffle(r)\n",
    "\n",
    "                    train_data_comb_tensor = train_data_comb_tensor[r]\n",
    "                    train_label_comb_tensor = train_label_comb_tensor[r]\n",
    "                    weight_prop_comb_tensor = weight_prop_comb_tensor[r]\n",
    "\n",
    "                    # train with weights\n",
    "                    train_loss, train_acc = train(train_data_comb_tensor, train_label_comb_tensor, model, criterion, optimizer,\n",
    "                                                  batch_size, epoch, use_cuda, loss_weights=weight_prop_comb_tensor)\n",
    "\n",
    "                end_time_werm = time.perf_counter()\n",
    "                training_time = end_time_werm - start_time_werm\n",
    "#                 print(f\"Dataset: {dataset}, Seed: {random_seed}, Time: {training_time}\")\n",
    "                time_per_epoch_werm[dataset].append(training_time)\n",
    "    print(f\"Dataset: {dataset}, Mean Training Time: {np.mean(time_per_epoch_werm[dataset])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c358c595",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
