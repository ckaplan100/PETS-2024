{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089a885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from train_utils import *\n",
    "from eval_utils import *\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2849ce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']=\"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES']= \"6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e50350",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "available_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3d78b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"use_cuda: {use_cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20883364",
   "metadata": {},
   "source": [
    "### run and save experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b859d75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_datasets = [\"purchase\", \"texas\", \"cifar\"]\n",
    "random_seeds = np.arange(5, 6)\n",
    "att_warmup_epochs = 0\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5384e279",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = {\n",
    "    \"cifar\": [1e-6, 1e-3, 1e-1, 1.],\n",
    "    \"purchase\": [1., 2., 3., 6., 10., 20.],\n",
    "    \"texas\": [1., 2., 3., 6., 10., 20.]\n",
    "}\n",
    "train_params_list = [\n",
    "    {\"training_style\": \"coin_flip\", \"att_loss\": \"mse\", \"non_member_loss_term\": True, \"ref_to_train_ratio\": 1.,\n",
    "     \"int_epochs_att\": 20, \"int_epochs_clf\": 1, \"batch_size_clf\": 128, \"batch_size_att\": 128},\n",
    "    {\"training_style\": \"coin_flip\", \"att_loss\": \"mse\", \"non_member_loss_term\": False, \"ref_to_train_ratio\": 1.,\n",
    "     \"int_epochs_att\": 20, \"int_epochs_clf\": 1, \"batch_size_clf\": 128, \"batch_size_att\": 128},\n",
    "#     {\"training_style\": \"standard\", \"att_loss\": \"bce\", \"non_member_loss_term\": True, \"ref_to_train_ratio\": 1.,\n",
    "#      \"int_epochs_att\": 20, \"int_epochs_clf\": 1, \"batch_size_clf\": 128, \"batch_size_att\": 128},\n",
    "#     {\"training_style\": \"standard\", \"att_loss\": \"bce\", \"non_member_loss_term\": False, \"ref_to_train_ratio\": 1.,\n",
    "#      \"int_epochs_att\": 20, \"int_epochs_clf\": 1, \"batch_size_clf\": 128, \"batch_size_att\": 128},   \n",
    "#     {\"training_style\": \"code\", \"att_loss\": \"mse\", \"non_member_loss_term\": False, \"ref_to_train_ratio\": 1.,\n",
    "#      \"int_epochs_att\": 76, \"int_epochs_clf\": 76, \"batch_size_clf\": 128, \"batch_size_att\": 128},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626308b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in run_datasets:\n",
    "    for train_params in train_params_list:\n",
    "        if train_params[\"training_style\"] == \"code\":\n",
    "            use_validation = True\n",
    "        else:\n",
    "            use_validation = True\n",
    "        \n",
    "        # get data segment proportions\n",
    "        if dataset == \"texas\":\n",
    "            num_features = 6169\n",
    "            ext_epochs = 20\n",
    "            data_is_numpy = True\n",
    "            if train_params[\"training_style\"] == \"code\":\n",
    "                train_classifier_ratio, train_attack_ratio, train_valid_ratio = 0.1485222040695084, 0.3, 0.2\n",
    "                num_batches_att = 50\n",
    "            else:\n",
    "                train_classifier_ratio = 0.15\n",
    "                train_test_ratio = 0.4\n",
    "                train_attack_ratio = train_classifier_ratio * train_params[\"ref_to_train_ratio\"]\n",
    "                train_valid_ratio = 1 - train_classifier_ratio - train_attack_ratio - train_test_ratio\n",
    "                num_batches_att = None\n",
    "        elif dataset == \"purchase\":\n",
    "            num_features = 600\n",
    "            ext_epochs = 40\n",
    "            data_is_numpy = True\n",
    "            if train_params[\"training_style\"] == \"code\":   \n",
    "                train_classifier_ratio, train_attack_ratio, train_valid_ratio = 0.1, 0.15, 0.25\n",
    "                num_batches_att = 52\n",
    "            else:\n",
    "                train_classifier_ratio = 0.1\n",
    "                train_test_ratio = 0.4\n",
    "                train_attack_ratio = train_classifier_ratio * train_params[\"ref_to_train_ratio\"]\n",
    "                train_valid_ratio = 1 - train_classifier_ratio - train_attack_ratio - train_test_ratio\n",
    "                num_batches_att = None\n",
    "        elif dataset == \"cifar\":\n",
    "            train_classifier_ratio, train_attack_ratio, train_valid_ratio = None, None, None\n",
    "            data_is_numpy = False\n",
    "            ext_epochs = 30\n",
    "            num_batches_att = None\n",
    "        else:\n",
    "            raise ValueError(\"not handled dataset\")\n",
    "        \n",
    "        # set run name\n",
    "        if train_params[\"non_member_loss_term\"]:\n",
    "            f_tag = \"nf\"\n",
    "        else:\n",
    "            f_tag = \"of\"\n",
    "        run_name = f'{train_params[\"training_style\"]}-{train_params[\"att_loss\"]}-{f_tag}-{train_params[\"ref_to_train_ratio\"]}-{ext_epochs}-{train_params[\"int_epochs_att\"]}-{train_params[\"int_epochs_clf\"]}'      \n",
    "        if train_params[\"batch_size_clf\"] != 128:\n",
    "            run_name += f\"-bs{train_params['batch_size_clf']}\"   \n",
    "        \n",
    "        for alpha in alphas[dataset]:\n",
    "            for random_seed in random_seeds:    \n",
    "                if random_seed in [5, 10]:\n",
    "                    load_randomization = True\n",
    "                else:\n",
    "                    load_randomization = False\n",
    "\n",
    "                print(dataset, run_name, alpha, random_seed)\n",
    "\n",
    "                # set random seed\n",
    "                set_seed(random_seed)\n",
    "\n",
    "                # set training params\n",
    "                best_valid_acc_state_dict = None\n",
    "                best_total_valid_loss_state_dict = None\n",
    "                best_valid_acc = 0.\n",
    "                best_valid_acc_epoch = -1\n",
    "                best_valid_loss = 1e5\n",
    "                best_valid_loss_epoch = -1\n",
    "\n",
    "                # load data\n",
    "                train_classifier_data, train_classifier_label, train_attack_data, train_attack_label, valid_data, valid_label, test_data, test_label = load_data(\n",
    "                    dataset=dataset, load_randomization=load_randomization, use_validation=use_validation,\n",
    "                    train_classifier_ratio=train_classifier_ratio, \n",
    "                    train_attack_ratio=train_attack_ratio, \n",
    "                    train_valid_ratio=train_valid_ratio\n",
    "                )\n",
    "\n",
    "                # instantiate training objects\n",
    "                if dataset == \"cifar\":\n",
    "                    model = resnet18(pretrained=False)\n",
    "                    model.fc = nn.Linear(512, 100)\n",
    "                else:\n",
    "                    model = TabularClassifier(num_features=num_features)\n",
    "                model = torch.nn.DataParallel(model).cuda()\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "                attack_model = InferenceAttack_HZ(100)\n",
    "                attack_model = torch.nn.DataParallel(attack_model).cuda()\n",
    "                if train_params[\"training_style\"] == \"code\":\n",
    "                    attack_criterion = nn.MSELoss()\n",
    "                    squared_loss = False\n",
    "                    log_loss = False\n",
    "                else:\n",
    "                    if train_params[\"att_loss\"] == \"mse\":                    \n",
    "                        attack_criterion = nn.MSELoss()\n",
    "                        squared_loss = True\n",
    "                        log_loss = False\n",
    "                        if train_params[\"training_style\"] == \"code\":\n",
    "                            squared_loss = False\n",
    "                    elif train_params[\"att_loss\"] == \"bce\":\n",
    "                        attack_criterion = nn.BCELoss()\n",
    "                        squared_loss = False\n",
    "                        log_loss = True\n",
    "                    else:\n",
    "                        raise ValueError(f\"unhandled attack loss: {train_params['att_loss']}\")\n",
    "                attack_optimizer = optim.Adam(attack_model.parameters(), lr=0.0001)\n",
    "                \n",
    "                for epoch in range(ext_epochs):\n",
    "                    if dataset in [\"purchase\", \"texas\"]:\n",
    "                        train_classifier_data_tensor = torch.from_numpy(train_classifier_data).type(torch.FloatTensor)\n",
    "                        train_classifier_label_tensor = torch.from_numpy(train_classifier_label).type(torch.LongTensor)\n",
    "\n",
    "                        train_attack_data_tensor = torch.from_numpy(train_attack_data).type(torch.FloatTensor)\n",
    "                        train_attack_label_tensor = torch.from_numpy(train_attack_label).type(torch.LongTensor)\n",
    "\n",
    "                        valid_data_tensor = torch.from_numpy(valid_data).type(torch.FloatTensor)\n",
    "                        valid_label_tensor = torch.from_numpy(valid_label).type(torch.LongTensor)\n",
    "\n",
    "                        test_data_tensor = torch.from_numpy(test_data).type(torch.FloatTensor)\n",
    "                        test_label_tensor = torch.from_numpy(test_label).type(torch.LongTensor)\n",
    "                    elif dataset == \"cifar\":\n",
    "                        train_classifier_data_tensor = train_classifier_data.type(torch.FloatTensor)\n",
    "                        train_classifier_label_tensor = train_classifier_label.type(torch.LongTensor)\n",
    "\n",
    "                        train_attack_data_tensor = train_attack_data.type(torch.FloatTensor)\n",
    "                        train_attack_label_tensor = train_attack_label.type(torch.LongTensor)\n",
    "\n",
    "                        valid_data_tensor = valid_data.type(torch.FloatTensor)\n",
    "                        valid_label_tensor = valid_label.type(torch.LongTensor)\n",
    "\n",
    "                        test_data_tensor = test_data.type(torch.FloatTensor)\n",
    "                        test_label_tensor = test_label.type(torch.LongTensor)\n",
    "                    else:\n",
    "                        raise ValueError(\"unhandled dataset\")\n",
    "                        \n",
    "                    r = np.arange(len(train_classifier_data_tensor))\n",
    "                    np.random.shuffle(r)\n",
    "                    train_classifier_data_tensor = train_classifier_data_tensor[r]\n",
    "                    train_classifier_label_tensor = train_classifier_label_tensor[r]\n",
    "                    \n",
    "                    r = np.arange(len(train_attack_data_tensor))\n",
    "                    np.random.shuffle(r)\n",
    "                    train_attack_data_tensor = train_attack_data_tensor[r]\n",
    "                    train_attack_label_tensor = train_attack_label_tensor[r]\n",
    "\n",
    "#                     print('\\nEpoch: [%d | %d]' % (epoch, ext_epochs))\n",
    "\n",
    "                    if epoch == 0:                 \n",
    "                        train_loss, train_acc = train(\n",
    "                            train_classifier_data_tensor, train_classifier_label_tensor, model, criterion, optimizer, \n",
    "                            train_params[\"batch_size_clf\"], epoch, use_cuda\n",
    "                        )\n",
    "\n",
    "                        for i in range(att_warmup_epochs):\n",
    "                            train_attack(\"standard\",\n",
    "                                train_classifier_data_tensor, train_classifier_label_tensor, \n",
    "                                train_attack_data_tensor, train_attack_label_tensor, \n",
    "                                model, attack_model, criterion, attack_criterion, optimizer, attack_optimizer,\n",
    "                                train_params[\"batch_size_att\"], epoch, use_cuda, None, None, i\n",
    "                            )\n",
    "                            \n",
    "                    else:\n",
    "                        mi_losses = []\n",
    "                        if train_params[\"training_style\"] == \"standard\":\n",
    "                            # train attack model\n",
    "                            for i in range(train_params[\"int_epochs_att\"]):\n",
    "                                at_loss, at_acc = train_attack(train_params[\"training_style\"],\n",
    "                                    train_classifier_data_tensor, train_classifier_label_tensor, \n",
    "                                    train_attack_data_tensor, train_attack_label_tensor, \n",
    "                                    model, attack_model, criterion, attack_criterion, optimizer, attack_optimizer,\n",
    "                                    train_params[\"batch_size_att\"], epoch, use_cuda, None, None, i)\n",
    "\n",
    "                            # these values correspond to the best attack model vs. previous classifier\n",
    "                            with torch.no_grad():\n",
    "                                # MI loss - members\n",
    "                                tr_mi_loss_o, tr_mi_loss_n = calculate_mi_loss(\n",
    "                                    train_classifier_data_tensor, train_classifier_label_tensor, model, attack_model, train_params[\"att_loss\"], is_ref_data=False)  \n",
    "                                # MI loss - reference - non-members\n",
    "                                at_mi_loss_o, at_mi_loss_n = calculate_mi_loss(\n",
    "                                    train_attack_data_tensor, train_attack_label_tensor, model, attack_model, train_params[\"att_loss\"], is_ref_data=True)\n",
    "                                # MI loss - non-members - test\n",
    "                                te_mi_loss_o, te_mi_loss_n = calculate_mi_loss(\n",
    "                                    test_data_tensor, test_label_tensor, model, attack_model, train_params[\"att_loss\"], is_ref_data=True)\n",
    "                                # MI loss - non-members - validation\n",
    "                                if use_validation:\n",
    "                                    vd_mi_loss_o, vd_mi_loss_n = calculate_mi_loss(\n",
    "                                        valid_data_tensor, valid_label_tensor, model, attack_model, train_params[\"att_loss\"], is_ref_data=True)\n",
    "                                else:\n",
    "                                    vd_mi_loss_o, vd_mi_loss_n = None, None\n",
    "\n",
    "                            # train classifier\n",
    "                            for i in range(train_params[\"int_epochs_clf\"]):\n",
    "                                tr_loss, tr_acc = train_privately(\n",
    "                                    training_style=train_params[\"training_style\"], train_data=train_classifier_data_tensor, labels=train_classifier_label_tensor,                                  \n",
    "                                    model=model, inference_model=attack_model, criterion=criterion, optimizer=optimizer, \n",
    "                                    batch_size=train_params[\"batch_size_clf\"], epoch=epoch, use_cuda=use_cuda, \n",
    "                                    num_batchs=None, skip_batch=None, alpha=alpha, \n",
    "                                    attack_data=train_attack_data_tensor, attack_labels=train_attack_label_tensor, \n",
    "                                    i=i, squared_loss=squared_loss, log_loss=log_loss, non_member_loss_term=train_params[\"non_member_loss_term\"]\n",
    "                                )\n",
    "\n",
    "                        elif train_params[\"training_style\"] == \"coin_flip\":\n",
    "                            clf_selections = 0\n",
    "                            internal_epochs = train_params[\"int_epochs_att\"] + train_params[\"int_epochs_clf\"]\n",
    "                            for r in range(internal_epochs):\n",
    "                                clf_train_proportion = train_params[\"int_epochs_clf\"] / internal_epochs\n",
    "                                model_train_sequence = []\n",
    "                                # batch size for classifier and attack model must be equal\n",
    "                                for i in range(int(train_classifier_data.shape[0] / train_params[\"batch_size_clf\"])):\n",
    "                                    model_to_train = np.random.binomial(n=1, p=clf_train_proportion)\n",
    "                                    model_train_sequence.append(model_to_train)\n",
    "                                    if model_to_train == 0:\n",
    "                                        at_loss, at_acc = train_attack(train_params[\"training_style\"],\n",
    "                                            train_classifier_data_tensor, train_classifier_label_tensor, \n",
    "                                            train_attack_data_tensor, train_attack_label_tensor, \n",
    "                                            model, attack_model, criterion, attack_criterion, optimizer, attack_optimizer,\n",
    "                                            train_params[\"batch_size_att\"], epoch, use_cuda, None, None, i)\n",
    "                                    else:\n",
    "                                        tr_loss, tr_acc = train_privately(\n",
    "                                            training_style=train_params[\"training_style\"], train_data=train_classifier_data_tensor, labels=train_classifier_label_tensor,                                  \n",
    "                                            model=model, inference_model=attack_model, criterion=criterion, optimizer=optimizer, \n",
    "                                            batch_size=train_params[\"batch_size_clf\"], epoch=epoch, use_cuda=use_cuda, \n",
    "                                            num_batchs=None, skip_batch=None, alpha=alpha, \n",
    "                                            attack_data=train_attack_data_tensor, attack_labels=train_attack_label_tensor, \n",
    "                                            i=i, squared_loss=squared_loss, log_loss=log_loss, non_member_loss_term=train_params[\"non_member_loss_term\"]\n",
    "                                        )\n",
    "                                        clf_selections += 1\n",
    "                            if verbose:\n",
    "                                print(\"clf selections\", clf_selections)\n",
    "\n",
    "                        elif train_params[\"training_style\"] == \"code\":\n",
    "                            code_train_metrics = []\n",
    "                            for i in range(76):\n",
    "                                at_loss, at_acc = train_attack(train_params[\"training_style\"],\n",
    "                                    train_classifier_data_tensor, train_classifier_label_tensor, train_attack_data_tensor,\n",
    "                                    train_attack_label_tensor, model, attack_model, criterion, attack_criterion, optimizer, attack_optimizer,\n",
    "                                    train_params[\"batch_size_att\"], epoch, use_cuda, num_batches_att, (i*num_batches_att)%150)\n",
    "                                tr_loss, tr_acc = train_privately(\n",
    "                                    training_style=train_params[\"training_style\"], train_data=train_classifier_data_tensor, labels=train_classifier_label_tensor,                                  \n",
    "                                    model=model, inference_model=attack_model, criterion=criterion, \n",
    "                                    optimizer=optimizer, batch_size=train_params[\"batch_size_clf\"],\n",
    "                                    epoch=epoch, use_cuda=use_cuda, num_batchs=2, skip_batch=(2*i)%152, alpha=alpha, \n",
    "                                    attack_data=train_attack_data_tensor, attack_labels=train_attack_label_tensor, \n",
    "                                    i=i, squared_loss=squared_loss, log_loss=log_loss, non_member_loss_term=train_params[\"non_member_loss_term\"]\n",
    "                                )\n",
    "                                # after this value it skips all batches and returns None, None\n",
    "                                if dataset == \"texas\" and i == 38:\n",
    "                                    code_train_metrics.append((tr_loss, tr_acc))\n",
    "                            if dataset == \"texas\":\n",
    "                                tr_loss, tr_acc = code_train_metrics[0]\n",
    "\n",
    "                        else:\n",
    "                            raise ValueError(f\"unhandled training style: {training_style}\")                            \n",
    "                            \n",
    "                    # get loss with data splits\n",
    "                    train_loss, train_acc = test(train_classifier_data_tensor, train_classifier_label_tensor, model, criterion, 128, epoch, use_cuda)\n",
    "\n",
    "                    ref_loss, ref_acc = test(train_attack_data_tensor, train_attack_label_tensor, model, criterion, 128, epoch, use_cuda)\n",
    "\n",
    "                    valid_loss, valid_acc = test(valid_data_tensor, valid_label_tensor, model, criterion, 128, epoch, use_cuda)\n",
    "\n",
    "                    test_loss, test_acc = test(test_data_tensor, test_label_tensor, model, criterion, 128, epoch, use_cuda)\n",
    "\n",
    "                    # get privacy attack metrics per epoch\n",
    "                    # test attack eval - training data\n",
    "                    corr_acc_train, conf_acc_train, entr_acc_train, mod_entr_acc_train = evaluation_metrics(\n",
    "                        model, train_classifier_data, train_classifier_label, test_data, test_label, data_is_numpy)\n",
    "\n",
    "                    # test attack eval - ref data\n",
    "                    corr_acc_ref, conf_acc_ref, entr_acc_ref, mod_entr_acc_ref = evaluation_metrics(\n",
    "                        model, train_attack_data, train_attack_label, test_data, test_label, data_is_numpy)\n",
    "                    \n",
    "#                     print(f'Train Acc: {train_acc}, Ref Acc: {ref_acc}, Valid Acc: {valid_acc}, Test Acc: {test_acc}')\n",
    "#                     print(f'Train Loss: {train_loss}, Ref Loss: {ref_loss}, Valid Loss: {valid_loss}, Test Loss: {test_loss}')\n",
    "#                     print(f\"Conf Attack Train: {conf_acc_train}, Conf Attack Ref: {conf_acc_ref}\")                      \n",
    "# #                     print(f'Gap Attack: {1/2 + (train_acc / 100 - test_acc / 100) / 2}')\n",
    "\n",
    "                    filename = f'seed{random_seed}/alpha{alpha}/train-{run_name}'\n",
    "\n",
    "                    if valid_acc.item() > best_valid_acc:\n",
    "                        best_valid_acc = valid_acc.item()\n",
    "                        best_valid_acc_epoch = epoch\n",
    "                        best_valid_acc_state_dict = deepcopy(model.state_dict())\n",
    "\n",
    "                    if valid_loss.item() < best_valid_loss:\n",
    "                        best_valid_loss = valid_loss.item()\n",
    "                        best_valid_loss_epoch = epoch\n",
    "                        best_total_valid_loss_state_dict = deepcopy(model.state_dict())\n",
    "                    \n",
    "                    save_checkpoint({        \n",
    "                            'epoch': epoch,\n",
    "                            'test_acc': test_acc,\n",
    "                            'test_loss': test_loss,\n",
    "                            'train_acc': train_acc,\n",
    "                            'train_loss': train_loss,\n",
    "                            'valid_acc': valid_acc,\n",
    "                            'valid_loss': valid_loss,\n",
    "                            'ref_acc': ref_acc,\n",
    "                            'ref_loss': ref_loss,\n",
    "                            'conf_acc_train': conf_acc_train,\n",
    "                            'conf_acc_ref': conf_acc_ref\n",
    "                        }, filename=filename, filename_end='Depoch%d'%epoch, checkpoint=f'./{dataset}_checkpoints')\n",
    "\n",
    "                # save best models\n",
    "                save_checkpoint(\n",
    "                    {\"state_dict\": best_valid_acc_state_dict}, \n",
    "                    checkpoint=f'./{dataset}_checkpoints',\n",
    "                    filename=filename,\n",
    "                    filename_end='best_valid_acc_model'\n",
    "                )\n",
    "                save_checkpoint(\n",
    "                    {\"state_dict\": best_total_valid_loss_state_dict}, \n",
    "                    checkpoint=f'./{dataset}_checkpoints',\n",
    "                    filename=filename,\n",
    "                    filename_end='best_valid_total_loss_model'\n",
    "                )\n",
    "\n",
    "#                 print(f\"Best Valid Acc: {best_valid_acc}, Epoch: {best_valid_acc_epoch}\")\n",
    "#                 print(f\"Best Valid Loss: {best_valid_loss}, Epoch: {best_valid_loss_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e75e77a",
   "metadata": {},
   "source": [
    "### evaluate per epoch training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4723e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_datasets = [\"purchase\", \"texas\", \"cifar\"]\n",
    "random_seeds = np.arange(5, 15)\n",
    "att_warmup_epochs = 0\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d0b28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = {\n",
    "    \"cifar\": [1.],\n",
    "    \"purchase\": [1.],\n",
    "    \"texas\": [1.]\n",
    "}\n",
    "train_params_list = [\n",
    "    {\"training_style\": \"coin_flip\", \"att_loss\": \"mse\", \"non_member_loss_term\": False, \"ref_to_train_ratio\": 1.,\n",
    "     \"int_epochs_att\": 20, \"int_epochs_clf\": 1, \"batch_size_clf\": 128, \"batch_size_att\": 128},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a80493f",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_per_epoch_advreg = {\"cifar\": [], \"purchase\": [], \"texas\": []}\n",
    "for dataset in run_datasets:\n",
    "    for train_params in train_params_list:\n",
    "        if train_params[\"training_style\"] == \"code\":\n",
    "            use_validation = True\n",
    "        else:\n",
    "            use_validation = True\n",
    "        \n",
    "        # get data segment proportions\n",
    "        if dataset == \"texas\":\n",
    "            num_features = 6169\n",
    "            ext_epochs = 1\n",
    "            data_is_numpy = True\n",
    "            if train_params[\"training_style\"] == \"code\":\n",
    "                train_classifier_ratio, train_attack_ratio, train_valid_ratio = 0.1485222040695084, 0.3, 0.2\n",
    "                num_batches_att = 1\n",
    "            else:\n",
    "                train_classifier_ratio = 0.15\n",
    "                train_test_ratio = 0.4\n",
    "                train_attack_ratio = train_classifier_ratio * train_params[\"ref_to_train_ratio\"]\n",
    "                train_valid_ratio = 1 - train_classifier_ratio - train_attack_ratio - train_test_ratio\n",
    "                num_batches_att = None\n",
    "        elif dataset == \"purchase\":\n",
    "            num_features = 600\n",
    "            ext_epochs = 1\n",
    "            data_is_numpy = True\n",
    "            if train_params[\"training_style\"] == \"code\":   \n",
    "                train_classifier_ratio, train_attack_ratio, train_valid_ratio = 0.1, 0.15, 0.25\n",
    "                num_batches_att = 52\n",
    "            else:\n",
    "                train_classifier_ratio = 0.1\n",
    "                train_test_ratio = 0.4\n",
    "                train_attack_ratio = train_classifier_ratio * train_params[\"ref_to_train_ratio\"]\n",
    "                train_valid_ratio = 1 - train_classifier_ratio - train_attack_ratio - train_test_ratio\n",
    "                num_batches_att = None\n",
    "        elif dataset == \"cifar\":\n",
    "            train_classifier_ratio, train_attack_ratio, train_valid_ratio = None, None, None\n",
    "            data_is_numpy = False\n",
    "            ext_epochs = 30\n",
    "            num_batches_att = None\n",
    "        else:\n",
    "            raise ValueError(\"not handled dataset\")\n",
    "        \n",
    "        # set run name\n",
    "        if train_params[\"non_member_loss_term\"]:\n",
    "            f_tag = \"nf\"\n",
    "        else:\n",
    "            f_tag = \"of\"\n",
    "        run_name = f'{train_params[\"training_style\"]}-{train_params[\"att_loss\"]}-{f_tag}-{train_params[\"ref_to_train_ratio\"]}-{ext_epochs}-{train_params[\"int_epochs_att\"]}-{train_params[\"int_epochs_clf\"]}'      \n",
    "        if train_params[\"batch_size_clf\"] != 128:\n",
    "            run_name += f\"-bs{train_params['batch_size_clf']}\"   \n",
    "        \n",
    "        for alpha in alphas[dataset]:\n",
    "            for random_seed in random_seeds:    \n",
    "                if random_seed in [5, 10]:\n",
    "                    load_randomization = True\n",
    "                else:\n",
    "                    load_randomization = False\n",
    "\n",
    "                print(dataset, run_name, alpha, random_seed)\n",
    "\n",
    "                # set random seed\n",
    "                set_seed(random_seed)\n",
    "\n",
    "                # set training params\n",
    "                best_valid_acc_state_dict = None\n",
    "                best_total_valid_loss_state_dict = None\n",
    "                best_valid_acc = 0.\n",
    "                best_valid_acc_epoch = -1\n",
    "                best_valid_loss = 1e5\n",
    "                best_valid_loss_epoch = -1\n",
    "\n",
    "                # load data\n",
    "                train_classifier_data, train_classifier_label, train_attack_data, train_attack_label, valid_data, valid_label, test_data, test_label = load_data(\n",
    "                    dataset=dataset, load_randomization=load_randomization, use_validation=use_validation,\n",
    "                    train_classifier_ratio=train_classifier_ratio, \n",
    "                    train_attack_ratio=train_attack_ratio, \n",
    "                    train_valid_ratio=train_valid_ratio\n",
    "                )\n",
    "\n",
    "                # instantiate training objects\n",
    "                if dataset == \"cifar\":\n",
    "                    model = resnet18(pretrained=False)\n",
    "                    model.fc = nn.Linear(512, 100)\n",
    "                else:\n",
    "                    model = TabularClassifier(num_features=num_features)\n",
    "                model = torch.nn.DataParallel(model).cuda()\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "                attack_model = InferenceAttack_HZ(100)\n",
    "                attack_model = torch.nn.DataParallel(attack_model).cuda()\n",
    "                if train_params[\"training_style\"] == \"code\":\n",
    "                    attack_criterion = nn.MSELoss()\n",
    "                    squared_loss = False\n",
    "                    log_loss = False\n",
    "                else:\n",
    "                    if train_params[\"att_loss\"] == \"mse\":                    \n",
    "                        attack_criterion = nn.MSELoss()\n",
    "                        squared_loss = True\n",
    "                        log_loss = False\n",
    "                        if train_params[\"training_style\"] == \"code\":\n",
    "                            squared_loss = False\n",
    "                    elif train_params[\"att_loss\"] == \"bce\":\n",
    "                        attack_criterion = nn.BCELoss()\n",
    "                        squared_loss = False\n",
    "                        log_loss = True\n",
    "                    else:\n",
    "                        raise ValueError(f\"unhandled attack loss: {train_params['att_loss']}\")\n",
    "                attack_optimizer = optim.Adam(attack_model.parameters(), lr=0.0001)\n",
    "                \n",
    "                start_time_advreg = time.perf_counter()\n",
    "                for epoch in range(ext_epochs):\n",
    "                    if dataset in [\"purchase\", \"texas\"]:\n",
    "                        train_classifier_data_tensor = torch.from_numpy(train_classifier_data).type(torch.FloatTensor)\n",
    "                        train_classifier_label_tensor = torch.from_numpy(train_classifier_label).type(torch.LongTensor)\n",
    "\n",
    "                        train_attack_data_tensor = torch.from_numpy(train_attack_data).type(torch.FloatTensor)\n",
    "                        train_attack_label_tensor = torch.from_numpy(train_attack_label).type(torch.LongTensor)\n",
    "\n",
    "                        valid_data_tensor = torch.from_numpy(valid_data).type(torch.FloatTensor)\n",
    "                        valid_label_tensor = torch.from_numpy(valid_label).type(torch.LongTensor)\n",
    "\n",
    "                        test_data_tensor = torch.from_numpy(test_data).type(torch.FloatTensor)\n",
    "                        test_label_tensor = torch.from_numpy(test_label).type(torch.LongTensor)\n",
    "                    elif dataset == \"cifar\":\n",
    "                        train_classifier_data_tensor = train_classifier_data.type(torch.FloatTensor)\n",
    "                        train_classifier_label_tensor = train_classifier_label.type(torch.LongTensor)\n",
    "\n",
    "                        train_attack_data_tensor = train_attack_data.type(torch.FloatTensor)\n",
    "                        train_attack_label_tensor = train_attack_label.type(torch.LongTensor)\n",
    "\n",
    "                        valid_data_tensor = valid_data.type(torch.FloatTensor)\n",
    "                        valid_label_tensor = valid_label.type(torch.LongTensor)\n",
    "\n",
    "                        test_data_tensor = test_data.type(torch.FloatTensor)\n",
    "                        test_label_tensor = test_label.type(torch.LongTensor)\n",
    "                    else:\n",
    "                        raise ValueError(\"unhandled dataset\")\n",
    "                        \n",
    "                    r = np.arange(len(train_classifier_data_tensor))\n",
    "                    np.random.shuffle(r)\n",
    "                    train_classifier_data_tensor = train_classifier_data_tensor[r]\n",
    "                    train_classifier_label_tensor = train_classifier_label_tensor[r]\n",
    "                    \n",
    "                    r = np.arange(len(train_attack_data_tensor))\n",
    "                    np.random.shuffle(r)\n",
    "                    train_attack_data_tensor = train_attack_data_tensor[r]\n",
    "                    train_attack_label_tensor = train_attack_label_tensor[r]\n",
    "\n",
    "#                     print('\\nEpoch: [%d | %d]' % (epoch, ext_epochs))\n",
    "\n",
    "                    mi_losses = []\n",
    "                    if train_params[\"training_style\"] == \"standard\":\n",
    "                        # train attack model\n",
    "                        for i in range(train_params[\"int_epochs_att\"]):\n",
    "                            at_loss, at_acc = train_attack(train_params[\"training_style\"],\n",
    "                                train_classifier_data_tensor, train_classifier_label_tensor, \n",
    "                                train_attack_data_tensor, train_attack_label_tensor, \n",
    "                                model, attack_model, criterion, attack_criterion, optimizer, attack_optimizer,\n",
    "                                train_params[\"batch_size_att\"], epoch, use_cuda, None, None, i)\n",
    "\n",
    "                        # these values correspond to the best attack model vs. previous classifier\n",
    "                        with torch.no_grad():\n",
    "                            # MI loss - members\n",
    "                            tr_mi_loss_o, tr_mi_loss_n = calculate_mi_loss(\n",
    "                                train_classifier_data_tensor, train_classifier_label_tensor, model, attack_model, train_params[\"att_loss\"], is_ref_data=False)  \n",
    "                            # MI loss - reference - non-members\n",
    "                            at_mi_loss_o, at_mi_loss_n = calculate_mi_loss(\n",
    "                                train_attack_data_tensor, train_attack_label_tensor, model, attack_model, train_params[\"att_loss\"], is_ref_data=True)\n",
    "                            # MI loss - non-members - test\n",
    "                            te_mi_loss_o, te_mi_loss_n = calculate_mi_loss(\n",
    "                                test_data_tensor, test_label_tensor, model, attack_model, train_params[\"att_loss\"], is_ref_data=True)\n",
    "                            # MI loss - non-members - validation\n",
    "                            if use_validation:\n",
    "                                vd_mi_loss_o, vd_mi_loss_n = calculate_mi_loss(\n",
    "                                    valid_data_tensor, valid_label_tensor, model, attack_model, train_params[\"att_loss\"], is_ref_data=True)\n",
    "                            else:\n",
    "                                vd_mi_loss_o, vd_mi_loss_n = None, None\n",
    "\n",
    "                        # train classifier\n",
    "                        for i in range(train_params[\"int_epochs_clf\"]):\n",
    "                            tr_loss, tr_acc = train_privately(\n",
    "                                training_style=train_params[\"training_style\"], train_data=train_classifier_data_tensor, labels=train_classifier_label_tensor,                                  \n",
    "                                model=model, inference_model=attack_model, criterion=criterion, optimizer=optimizer, \n",
    "                                batch_size=train_params[\"batch_size_clf\"], epoch=epoch, use_cuda=use_cuda, \n",
    "                                num_batchs=None, skip_batch=None, alpha=alpha, \n",
    "                                attack_data=train_attack_data_tensor, attack_labels=train_attack_label_tensor, \n",
    "                                i=i, squared_loss=squared_loss, log_loss=log_loss, non_member_loss_term=train_params[\"non_member_loss_term\"]\n",
    "                            )\n",
    "\n",
    "                    elif train_params[\"training_style\"] == \"coin_flip\":\n",
    "                        clf_selections = 0\n",
    "                        internal_epochs = train_params[\"int_epochs_att\"] + train_params[\"int_epochs_clf\"]\n",
    "                        for r in range(internal_epochs):\n",
    "                            clf_train_proportion = train_params[\"int_epochs_clf\"] / internal_epochs\n",
    "                            model_train_sequence = []\n",
    "                            # batch size for classifier and attack model must be equal\n",
    "                            for i in range(int(train_classifier_data.shape[0] / train_params[\"batch_size_clf\"])):\n",
    "                                model_to_train = np.random.binomial(n=1, p=clf_train_proportion)\n",
    "                                model_train_sequence.append(model_to_train)\n",
    "                                if model_to_train == 0:\n",
    "                                    at_loss, at_acc = train_attack(train_params[\"training_style\"],\n",
    "                                        train_classifier_data_tensor, train_classifier_label_tensor, \n",
    "                                        train_attack_data_tensor, train_attack_label_tensor, \n",
    "                                        model, attack_model, criterion, attack_criterion, optimizer, attack_optimizer,\n",
    "                                        train_params[\"batch_size_att\"], epoch, use_cuda, None, None, i)\n",
    "                                else:\n",
    "                                    tr_loss, tr_acc = train_privately(\n",
    "                                        training_style=train_params[\"training_style\"], train_data=train_classifier_data_tensor, labels=train_classifier_label_tensor,                                  \n",
    "                                        model=model, inference_model=attack_model, criterion=criterion, optimizer=optimizer, \n",
    "                                        batch_size=train_params[\"batch_size_clf\"], epoch=epoch, use_cuda=use_cuda, \n",
    "                                        num_batchs=None, skip_batch=None, alpha=alpha, \n",
    "                                        attack_data=train_attack_data_tensor, attack_labels=train_attack_label_tensor, \n",
    "                                        i=i, squared_loss=squared_loss, log_loss=log_loss, non_member_loss_term=train_params[\"non_member_loss_term\"]\n",
    "                                    )\n",
    "                                    clf_selections += 1\n",
    "                        if verbose:\n",
    "                            print(\"clf selections\", clf_selections)\n",
    "\n",
    "                    elif train_params[\"training_style\"] == \"code\":\n",
    "                        code_train_metrics = []\n",
    "                        for i in range(76):\n",
    "                            at_loss, at_acc = train_attack(train_params[\"training_style\"],\n",
    "                                train_classifier_data_tensor, train_classifier_label_tensor, train_attack_data_tensor,\n",
    "                                train_attack_label_tensor, model, attack_model, criterion, attack_criterion, optimizer, attack_optimizer,\n",
    "                                train_params[\"batch_size_att\"], epoch, use_cuda, num_batches_att, (i*num_batches_att)%150)\n",
    "                            tr_loss, tr_acc = train_privately(\n",
    "                                training_style=train_params[\"training_style\"], train_data=train_classifier_data_tensor, labels=train_classifier_label_tensor,                                  \n",
    "                                model=model, inference_model=attack_model, criterion=criterion, \n",
    "                                optimizer=optimizer, batch_size=train_params[\"batch_size_clf\"],\n",
    "                                epoch=epoch, use_cuda=use_cuda, num_batchs=2, skip_batch=(2*i)%152, alpha=alpha, \n",
    "                                attack_data=train_attack_data_tensor, attack_labels=train_attack_label_tensor, \n",
    "                                i=i, squared_loss=squared_loss, log_loss=log_loss, non_member_loss_term=train_params[\"non_member_loss_term\"]\n",
    "                            )\n",
    "\n",
    "                    else:\n",
    "                        raise ValueError(f\"unhandled training style: {training_style}\")\n",
    "                end_time_advreg = time.perf_counter()\n",
    "                training_time = end_time_advreg - start_time_advreg\n",
    "#                 print(f\"Dataset: {dataset}, Seed: {random_seed}, Time: {training_time}\")\n",
    "                time_per_epoch_advreg[dataset].append(training_time)\n",
    "    print(f\"Dataset: {dataset}, Mean Training Time: {np.mean(time_per_epoch_advreg[dataset])}\")\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9355a6d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
