{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb09307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from train_utils import *\n",
    "from eval_utils import *\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2598cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']=\"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES']= \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6510802",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "available_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b321d93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"use_cuda: {use_cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be1ce06",
   "metadata": {},
   "source": [
    "### run and save experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70133878",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_datasets = [\"purchase\", \"texas\", \"cifar\"]\n",
    "random_seeds = list(np.arange(5, 6))\n",
    "use_validation = True\n",
    "mmd_weights = [0.1, 0.2, 0.35, 0.7, 1.5]\n",
    "ref_to_train_ratio = 1.\n",
    "mmd_scale = 1.\n",
    "start_mmd_epochs = [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971c1656",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in run_datasets:\n",
    "    for random_seed in random_seeds:\n",
    "        for mmd_weight in mmd_weights:\n",
    "            for start_mmd_epoch in start_mmd_epochs:                    \n",
    "\n",
    "                set_seed(random_seed)\n",
    "\n",
    "                run_name = f\"weight{mmd_weight}-rttr{ref_to_train_ratio}\"\n",
    "\n",
    "                if start_mmd_epoch == -1:\n",
    "                    run_name += \"-no-warmup\"\n",
    "\n",
    "                print(dataset, random_seed, run_name)\n",
    "\n",
    "                best_valid_acc_state_dict = None\n",
    "                best_total_valid_loss_state_dict = None\n",
    "                best_valid_acc = 0.\n",
    "                best_valid_acc_epoch = -1\n",
    "                best_valid_loss = 1e5\n",
    "                best_valid_loss_epoch = -1\n",
    "\n",
    "                if random_seed in [5, 10]:\n",
    "                    load_randomization = True\n",
    "                else:\n",
    "                    load_randomization = False\n",
    "\n",
    "                if dataset == \"texas\":\n",
    "                    if start_mmd_epoch == -1:\n",
    "                        epochs = 16\n",
    "                    else:\n",
    "                        epochs = 8\n",
    "                    num_features = 6169\n",
    "                    train_classifier_ratio = 0.15\n",
    "                    train_test_ratio = 0.4\n",
    "                    train_attack_ratio = train_classifier_ratio * ref_to_train_ratio\n",
    "                    train_valid_ratio = 1 - train_classifier_ratio - train_attack_ratio - train_test_ratio\n",
    "                    data_is_numpy = True\n",
    "                    batch_size = 512\n",
    "                elif dataset == \"purchase\":\n",
    "                    if start_mmd_epoch == -1:\n",
    "                        epochs = 40\n",
    "                    else:\n",
    "                        epochs = 20\n",
    "                    num_features = 600\n",
    "                    train_classifier_ratio = 0.1\n",
    "                    train_test_ratio = 0.4\n",
    "                    train_attack_ratio = train_classifier_ratio * ref_to_train_ratio\n",
    "                    train_valid_ratio = 1 - train_classifier_ratio - train_attack_ratio - train_test_ratio\n",
    "                    data_is_numpy = True\n",
    "                    batch_size = 512\n",
    "                elif dataset == \"cifar\":\n",
    "                    if start_mmd_epoch == -1:\n",
    "                        epochs = 16\n",
    "                    else:\n",
    "                        epochs = 8\n",
    "                    train_classifier_ratio, train_attack_ratio, train_valid_ratio = None, None, None\n",
    "                    data_is_numpy = False  \n",
    "                    batch_size = 512\n",
    "                else:\n",
    "                    raise ValueError(\"not handled dataset\")\n",
    "\n",
    "                train_classifier_data, train_classifier_label, train_attack_data, train_attack_label, valid_data, valid_label, test_data, test_label = load_data(\n",
    "                    dataset=dataset, load_randomization=load_randomization, use_validation=use_validation,\n",
    "                    train_classifier_ratio=train_classifier_ratio, \n",
    "                    train_attack_ratio=train_attack_ratio, \n",
    "                    train_valid_ratio=train_valid_ratio\n",
    "                )\n",
    "                if dataset == \"cifar\":\n",
    "                    model = resnet18(pretrained=False)\n",
    "                    model.fc = nn.Linear(512, 100)\n",
    "                else:\n",
    "                    model = TabularClassifier(num_features=num_features)\n",
    "                model = model.cuda()\n",
    "                criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "                optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "                \n",
    "                for epoch in range(epochs):\n",
    "                    if dataset in [\"purchase\", \"texas\"]:\n",
    "                        train_classifier_data_tensor = torch.from_numpy(train_classifier_data).type(torch.FloatTensor)\n",
    "                        train_classifier_label_tensor = torch.from_numpy(train_classifier_label).type(torch.LongTensor)\n",
    "\n",
    "                        train_attack_data_tensor = torch.from_numpy(train_attack_data).type(torch.FloatTensor)\n",
    "                        train_attack_label_tensor = torch.from_numpy(train_attack_label).type(torch.LongTensor)\n",
    "\n",
    "                        valid_data_tensor = torch.from_numpy(valid_data).type(torch.FloatTensor)\n",
    "                        valid_label_tensor = torch.from_numpy(valid_label).type(torch.LongTensor)\n",
    "\n",
    "                        test_data_tensor = torch.from_numpy(test_data).type(torch.FloatTensor)\n",
    "                        test_label_tensor = torch.from_numpy(test_label).type(torch.LongTensor)\n",
    "                    elif dataset == \"cifar\":\n",
    "                        train_classifier_data_tensor = train_classifier_data.type(torch.FloatTensor)\n",
    "                        train_classifier_label_tensor = train_classifier_label.type(torch.LongTensor)\n",
    "\n",
    "                        train_attack_data_tensor = train_attack_data.type(torch.FloatTensor)\n",
    "                        train_attack_label_tensor = train_attack_label.type(torch.LongTensor)\n",
    "\n",
    "                        valid_data_tensor = valid_data.type(torch.FloatTensor)\n",
    "                        valid_label_tensor = valid_label.type(torch.LongTensor)\n",
    "\n",
    "                        test_data_tensor = test_data.type(torch.FloatTensor)\n",
    "                        test_label_tensor = test_label.type(torch.LongTensor)\n",
    "                    else:\n",
    "                        raise ValueError(\"unhandled dataset\")\n",
    "                    \n",
    "                    r = np.arange(len(train_classifier_data_tensor))\n",
    "                    np.random.shuffle(r)\n",
    "                    train_classifier_data_tensor = train_classifier_data_tensor[r]\n",
    "                    train_classifier_label_tensor = train_classifier_label_tensor[r]\n",
    "                    \n",
    "                    r = np.arange(len(train_attack_data_tensor))\n",
    "                    np.random.shuffle(r)\n",
    "                    train_attack_data_tensor = train_attack_data_tensor[r]\n",
    "                    train_attack_label_tensor = train_attack_label_tensor[r]\n",
    "\n",
    "#                     print('\\nEpoch: [%d | %d]' % (epoch, epochs))\n",
    "\n",
    "                    # train with weights\n",
    "                    train_loss, train_acc = train(\n",
    "                        train_classifier_data_tensor, train_classifier_label_tensor, model, criterion, optimizer, \n",
    "                        batch_size, epoch, use_cuda, \n",
    "                        mmd_weight=mmd_weight, mmd_scale=mmd_scale, start_mmd_epoch=start_mmd_epoch, \n",
    "                        ref_data=train_attack_data_tensor, ref_labels=train_attack_label_tensor, \n",
    "                        unique_labels=True, mmd_ref_term=True\n",
    "                    )\n",
    "\n",
    "                    # get loss with data splits\n",
    "                    train_loss, train_acc = test(train_classifier_data_tensor, train_classifier_label_tensor, model, criterion, 128, epoch, use_cuda)\n",
    "\n",
    "                    ref_loss, ref_acc = test(train_attack_data_tensor, train_attack_label_tensor, model, criterion, 128, epoch, use_cuda)\n",
    "\n",
    "                    valid_loss, valid_acc = test(valid_data_tensor, valid_label_tensor, model, criterion, 128, epoch, use_cuda)\n",
    "\n",
    "                    test_loss, test_acc = test(test_data_tensor, test_label_tensor, model, criterion, 128, epoch, use_cuda)\n",
    "\n",
    "                    # get privacy attack metrics per epoch\n",
    "                    # test attack eval - training data\n",
    "                    corr_acc_train, conf_acc_train, entr_acc_train, mod_entr_acc_train = evaluation_metrics(\n",
    "                        model, train_classifier_data, train_classifier_label, test_data, test_label, data_is_numpy)\n",
    "\n",
    "                    # test attack eval - ref data\n",
    "                    corr_acc_ref, conf_acc_ref, entr_acc_ref, mod_entr_acc_ref = evaluation_metrics(\n",
    "                        model, train_attack_data, train_attack_label, test_data, test_label, data_is_numpy)\n",
    "                    \n",
    "#                     print(f'Train Acc: {train_acc}, Ref Acc: {ref_acc}, Valid Acc: {valid_acc}, Test Acc: {test_acc}')\n",
    "#                     print(f'Train Loss: {train_loss}, Ref Loss: {ref_loss}, Valid Loss: {valid_loss}, Test Loss: {test_loss}')\n",
    "#                     print(f\"Conf Attack Train: {conf_acc_train}, Conf Attack Ref: {conf_acc_ref}\")                      \n",
    "#                     print(f'Gap Attack: {1/2 + (train_acc / 100 - test_acc / 100) / 2}')\n",
    "\n",
    "                    filename = f'seed{random_seed}/mmd-regularization/train-{run_name}'\n",
    "\n",
    "                    if valid_acc.item() > best_valid_acc:\n",
    "                        best_valid_acc = valid_acc.item()\n",
    "                        best_valid_acc_epoch = epoch\n",
    "                        best_valid_acc_state_dict = deepcopy(model.state_dict())\n",
    "\n",
    "                    if valid_loss.item() < best_valid_loss:\n",
    "                        best_valid_loss = valid_loss.item()\n",
    "                        best_valid_loss_epoch = epoch\n",
    "                        best_total_valid_loss_state_dict = deepcopy(model.state_dict())\n",
    "                    \n",
    "                    save_checkpoint({        \n",
    "                            'epoch': epoch,\n",
    "                            'test_acc': test_acc,\n",
    "                            'test_loss': test_loss,\n",
    "                            'train_acc': train_acc,\n",
    "                            'train_loss': train_loss,\n",
    "                            'valid_acc': valid_acc,\n",
    "                            'valid_loss': valid_loss,\n",
    "                            'ref_acc': ref_acc,\n",
    "                            'ref_loss': ref_loss,\n",
    "                            'conf_acc_train': conf_acc_train,\n",
    "                            'conf_acc_ref': conf_acc_ref\n",
    "                        }, filename=filename, filename_end='Depoch%d'%epoch, checkpoint=f'./{dataset}_checkpoints')\n",
    "\n",
    "                # save best models\n",
    "                save_checkpoint(\n",
    "                    {\"state_dict\": best_valid_acc_state_dict}, \n",
    "                    checkpoint=f'./{dataset}_checkpoints',\n",
    "                    filename=filename,\n",
    "                    filename_end='best_valid_acc_model'\n",
    "                )\n",
    "                save_checkpoint(\n",
    "                    {\"state_dict\": best_total_valid_loss_state_dict}, \n",
    "                    checkpoint=f'./{dataset}_checkpoints',\n",
    "                    filename=filename,\n",
    "                    filename_end='best_valid_total_loss_model'\n",
    "                )\n",
    "\n",
    "#                 print(f\"Best Valid Acc: {best_valid_acc}, Epoch: {best_valid_acc_epoch}\")\n",
    "#                 print(f\"Best Valid Loss: {best_valid_loss}, Epoch: {best_valid_loss_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce199234",
   "metadata": {},
   "source": [
    "### evaluate per epoch training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f053b4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_datasets = [\"purchase\", \"texas\", \"cifar\"]\n",
    "random_seeds = list(np.arange(5, 15))\n",
    "use_validation = True\n",
    "mmd_weights = [0.7]\n",
    "ref_to_train_ratio = 1.\n",
    "mmd_scale = 1.\n",
    "start_mmd_epochs = [-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fcfd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_per_epoch_mmd = {\"cifar\": [], \"purchase\": [], \"texas\": []}\n",
    "for dataset in run_datasets:\n",
    "    for random_seed in random_seeds:\n",
    "        for mmd_weight in mmd_weights:\n",
    "            for start_mmd_epoch in start_mmd_epochs:                    \n",
    "\n",
    "                set_seed(random_seed)\n",
    "\n",
    "                run_name = f\"weight{mmd_weight}-rttr{ref_to_train_ratio}\"\n",
    "\n",
    "                if start_mmd_epoch == -1:\n",
    "                    run_name += \"-no-warmup\"\n",
    "\n",
    "                print(dataset, random_seed, run_name)\n",
    "\n",
    "                best_valid_acc_state_dict = None\n",
    "                best_total_valid_loss_state_dict = None\n",
    "                best_valid_acc = 0.\n",
    "                best_valid_acc_epoch = -1\n",
    "                best_valid_loss = 1e5\n",
    "                best_valid_loss_epoch = -1\n",
    "\n",
    "                if random_seed in [5, 10]:\n",
    "                    load_randomization = True\n",
    "                else:\n",
    "                    load_randomization = False\n",
    "\n",
    "                if dataset == \"texas\":\n",
    "                    if start_mmd_epoch == -1:\n",
    "                        epochs = 1\n",
    "                    else:\n",
    "                        epochs = 1\n",
    "                    num_features = 6169\n",
    "                    train_classifier_ratio = 0.15\n",
    "                    train_test_ratio = 0.4\n",
    "                    train_attack_ratio = train_classifier_ratio * ref_to_train_ratio\n",
    "                    train_valid_ratio = 1 - train_classifier_ratio - train_attack_ratio - train_test_ratio\n",
    "                    data_is_numpy = True\n",
    "                    batch_size = 512\n",
    "                elif dataset == \"purchase\":\n",
    "                    if start_mmd_epoch == -1:\n",
    "                        epochs = 1\n",
    "                    else:\n",
    "                        epochs = 1\n",
    "                    num_features = 600\n",
    "                    train_classifier_ratio = 0.1\n",
    "                    train_test_ratio = 0.4\n",
    "                    train_attack_ratio = train_classifier_ratio * ref_to_train_ratio\n",
    "                    train_valid_ratio = 1 - train_classifier_ratio - train_attack_ratio - train_test_ratio\n",
    "                    data_is_numpy = True\n",
    "                    batch_size = 512\n",
    "                elif dataset == \"cifar\":\n",
    "                    if start_mmd_epoch == -1:\n",
    "                        epochs = 1\n",
    "                    else:\n",
    "                        epochs = 1\n",
    "                    train_classifier_ratio, train_attack_ratio, train_valid_ratio = None, None, None\n",
    "                    data_is_numpy = False  \n",
    "                    batch_size = 512\n",
    "                else:\n",
    "                    raise ValueError(\"not handled dataset\")\n",
    "\n",
    "                train_classifier_data, train_classifier_label, train_attack_data, train_attack_label, valid_data, valid_label, test_data, test_label = load_data(\n",
    "                    dataset=dataset, load_randomization=load_randomization, use_validation=use_validation,\n",
    "                    train_classifier_ratio=train_classifier_ratio, \n",
    "                    train_attack_ratio=train_attack_ratio, \n",
    "                    train_valid_ratio=train_valid_ratio\n",
    "                )\n",
    "                if dataset == \"cifar\":\n",
    "                    model = resnet18(pretrained=False)\n",
    "                    model.fc = nn.Linear(512, 100)\n",
    "                else:\n",
    "                    model = TabularClassifier(num_features=num_features)\n",
    "                model = model.cuda()\n",
    "                criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "                optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "                \n",
    "                start_time_mmd = time.perf_counter()\n",
    "                for epoch in range(epochs):\n",
    "                    if dataset in [\"purchase\", \"texas\"]:\n",
    "                        train_classifier_data_tensor = torch.from_numpy(train_classifier_data).type(torch.FloatTensor)\n",
    "                        train_classifier_label_tensor = torch.from_numpy(train_classifier_label).type(torch.LongTensor)\n",
    "\n",
    "                        train_attack_data_tensor = torch.from_numpy(train_attack_data).type(torch.FloatTensor)\n",
    "                        train_attack_label_tensor = torch.from_numpy(train_attack_label).type(torch.LongTensor)\n",
    "\n",
    "                        valid_data_tensor = torch.from_numpy(valid_data).type(torch.FloatTensor)\n",
    "                        valid_label_tensor = torch.from_numpy(valid_label).type(torch.LongTensor)\n",
    "\n",
    "                        test_data_tensor = torch.from_numpy(test_data).type(torch.FloatTensor)\n",
    "                        test_label_tensor = torch.from_numpy(test_label).type(torch.LongTensor)\n",
    "                    elif dataset == \"cifar\":\n",
    "                        train_classifier_data_tensor = train_classifier_data.type(torch.FloatTensor)\n",
    "                        train_classifier_label_tensor = train_classifier_label.type(torch.LongTensor)\n",
    "\n",
    "                        train_attack_data_tensor = train_attack_data.type(torch.FloatTensor)\n",
    "                        train_attack_label_tensor = train_attack_label.type(torch.LongTensor)\n",
    "\n",
    "                        valid_data_tensor = valid_data.type(torch.FloatTensor)\n",
    "                        valid_label_tensor = valid_label.type(torch.LongTensor)\n",
    "\n",
    "                        test_data_tensor = test_data.type(torch.FloatTensor)\n",
    "                        test_label_tensor = test_label.type(torch.LongTensor)\n",
    "                    else:\n",
    "                        raise ValueError(\"unhandled dataset\")\n",
    "                    \n",
    "                    r = np.arange(len(train_classifier_data_tensor))\n",
    "                    np.random.shuffle(r)\n",
    "                    train_classifier_data_tensor = train_classifier_data_tensor[r]\n",
    "                    train_classifier_label_tensor = train_classifier_label_tensor[r]\n",
    "                    \n",
    "                    r = np.arange(len(train_attack_data_tensor))\n",
    "                    np.random.shuffle(r)\n",
    "                    train_attack_data_tensor = train_attack_data_tensor[r]\n",
    "                    train_attack_label_tensor = train_attack_label_tensor[r]\n",
    "\n",
    "                    # train with weights\n",
    "                    train_loss, train_acc = train(\n",
    "                        train_classifier_data_tensor, train_classifier_label_tensor, model, criterion, optimizer, \n",
    "                        batch_size, epoch, use_cuda, \n",
    "                        mmd_weight=mmd_weight, mmd_scale=mmd_scale, start_mmd_epoch=start_mmd_epoch, \n",
    "                        ref_data=train_attack_data_tensor, ref_labels=train_attack_label_tensor, \n",
    "                        unique_labels=True, mmd_ref_term=True\n",
    "                    )\n",
    "\n",
    "                end_time_mmd = time.perf_counter()\n",
    "                training_time = end_time_mmd - start_time_mmd\n",
    "#                 print(f\"Dataset: {dataset}, Seed: {random_seed}, Time: {training_time}\")\n",
    "                time_per_epoch_mmd[dataset].append(training_time)\n",
    "    print(f\"Dataset: {dataset}, Mean Training Time: {np.mean(time_per_epoch_mmd[dataset])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0106d2c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
